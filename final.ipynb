{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malicious URL Detection with Robust Features\n",
    "\n",
    "## By: Lee Kochav & Eliel Gez\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    " \n",
    "import os.path, time, random, sys, itertools, pickle, ast\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import concurrent.futures\n",
    "from nslookup import Nslookup\n",
    "from hashlib import new\n",
    "from re import split\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from riskiq.api import Client\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Length of Domains and Number of Consecutive Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "'''\n",
    "Length of domains and number of consecutivecharacters\n",
    "'''\n",
    "def consecutive_characters(str):\n",
    "    l = len(str)\n",
    "    count = 0\n",
    "    res = str[0]\n",
    "    for i in range(l):   \n",
    "        cur_count = 1\n",
    "        for j in range(i + 1, l):\n",
    "            if (str[i] != str[j]):\n",
    "                break\n",
    "            cur_count += 1\n",
    "        if cur_count > count:\n",
    "            count = cur_count\n",
    "            res = str[i]\n",
    "    return count\n",
    "\n",
    "\n",
    "df= pd.read_csv('./newMalWithAds.csv')\n",
    "\n",
    "domains= df['domain']\n",
    "results = []\n",
    "con_chars = []\n",
    "for index, domain in enumerate(domains):\n",
    "    print(f'Run {index}/{len(domains)}')\n",
    "    new_domain = urlparse(domain).netloc\n",
    "    results.append(len(new_domain))\n",
    "    con_chars.append(consecutive_characters(new_domain))\n",
    "\n",
    "#dom_length\n",
    "df['dom_length']=results\n",
    "#consecutive_chars\n",
    "df['consecutive_chars'] = con_chars\n",
    "df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "try:\n",
    "    df.to_csv('./newMalWithAds.csv')\n",
    "except:\n",
    "    textfile = open(\"LengthOfDomains.txt\", \"w\")\n",
    "    for element in results:\n",
    "        textfile.write(element + \"\\n\")\n",
    "    textfile.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# change the path if needed \n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "import math\n",
    "\n",
    "\n",
    "#df = pd.read_csv('Final_newData_withFeatures.csv')\n",
    "df= pd.read_csv('./newMalWithAds.csv')\n",
    "urls = df['domain']\n",
    "\n",
    "entropies = []\n",
    "\n",
    "for index, url in enumerate(urls):\n",
    "    domain=\"\"\n",
    "    if url[:4] == 'http':\n",
    "        domain  = urlparse(url).netloc\n",
    "    else:\n",
    "        domain = urlparse('http://'+url).netloc\n",
    "    \n",
    "    entropy  = 0\n",
    "    str_len  = len(domain)\n",
    "    chars    = defaultdict(int)\n",
    "    for char in domain:\n",
    "        chars[char] += 1\n",
    "    for char in domain:\n",
    "        pj       = (chars[char]/str_len)\n",
    "        entropy  += pj*math.log(pj,2)\n",
    "    entropies.append((-1)*entropy)\n",
    "\n",
    "#entropy\n",
    "df['entropy'] = pd.Series(entropies)\n",
    "df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "#df=df[df['length'] != -1]\n",
    "try:    \n",
    "    df.to_csv('./newMalWithAds.csv')\n",
    "except:\n",
    "    pass\n",
    "#df.to_csv('superFinal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Number of Passive DNS records from Riskiq api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "API_SECRET=\"8c1cf1a737c204b9\"\n",
    "API_KEY=\"/XOX1muJq/3Wy9aKSQSutVFYFtl2XVTE\"\n",
    "client = Client(API_SECRET, API_KEY)\n",
    "\n",
    "df= pd.read_csv('./newMalWithAds.csv')\n",
    "domains= df['domain']\n",
    "errors=0\n",
    "results=[]\n",
    "\n",
    "\n",
    "for index, domain in enumerate(domains):\n",
    "    print(f'Run {index}/{len(domains)}')\n",
    "    new_domain = urlparse(domain).netloc\n",
    "    try:\n",
    "        response=client.get_dns_data_by_name(new_domain,rrtype=None, maxresults=1000)\n",
    "        results.append(int(response['recordCount']))\n",
    "    except:\n",
    "        errors+=1\n",
    "        print(f'Error number {errors}')\n",
    "        results.append(-1)\n",
    "    \n",
    "\n",
    "#passive dns   \n",
    "df['passive_dns']=results\n",
    "df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "try:\n",
    "    df.to_csv('./newMalWithAds.csv')\n",
    "except:\n",
    "    textfile = open(\"CollectPDNS.txt\", \"w\")\n",
    "    for element in results:\n",
    "        textfile.write(element + \"\\n\")\n",
    "    textfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect SSL Certificate data from riskiq api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_SECRET=\"8c1cf1a737c204b9\"\n",
    "API_KEY=\"/XOX1muJq/3Wy9aKSQSutVFYFtl2XVTE\"\n",
    "\n",
    "client = Client(API_SECRET, API_KEY)\n",
    "\n",
    "df= pd.read_csv('./newMalWithAds.csv')\n",
    "domains= df['domain']\n",
    "errors=0\n",
    "results=[]\n",
    "\n",
    "url_to_scan = ''\n",
    "      \n",
    "url = f'https://api.riskiq.net/pt/v2/ssl-certificate/history?query={url_to_scan}'\n",
    "headers = {'Accept': 'application/json','Authorization': 'Basic ZWxjeWJlcjEyMzRAZ21haWwuY29tOmE2ZmZhZjJmY2I4ZDlkOTcwNDJhNmJhY2E3Njc1M2MxZjE0NDQ2ZTdhNTliNTkyZWFiNjAxNGVkMjVjMWM5YTU='}\n",
    "\n",
    "for index, domain in enumerate(domains):\n",
    "    new_domain = urlparse(domain).netloc\n",
    "    print(f'Run {index}/{len(domains)}')\n",
    "    url = f'https://api.riskiq.net/pt/v2/ssl-certificate/history?query={new_domain}'\n",
    "    res = requests.get(url, headers=headers)\n",
    "    data =json.loads(res.text)\n",
    "    content = data['results']\n",
    "    final=0\n",
    "    if content != [] :\n",
    "        first_seen= int(datetime.fromisoformat(content[0]['firstSeen']).timestamp())\n",
    "        last_seen = int (datetime.fromisoformat(content[0]['lastSeen']).timestamp())\n",
    "        final=last_seen-first_seen\n",
    "    results.append(final)\n",
    "\n",
    "#ssl_time\n",
    "df['ssl_time']=results\n",
    "df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "try:\n",
    "    df.to_csv('./newMalWithAds.csv')\n",
    "except:\n",
    "    textfile = open(\"CollectSSL.txt\", \"w\")\n",
    "    for element in results:\n",
    "        textfile.write(element + \"\\n\")\n",
    "    textfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect TTL and IP addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Change path as needed\n",
    "malicious_dom_file = './Datasets/domains2.csv'\n",
    "df_domains = pd.read_csv(malicious_dom_file)\n",
    "df= df_domains['domain']\n",
    "print(df.head())\n",
    "out_csv = 'newMalWithAds.csv'\n",
    "is_active = []\n",
    "ttls = []\n",
    "ips = []\n",
    "num_of_ips = []\n",
    "\n",
    "failed_resols = 0\n",
    "\n",
    "for index,row in enumerate(df):\n",
    "    try:\n",
    "        domain = urlparse(row).netloc\n",
    "        print(domain)\n",
    "        dns_query = Nslookup(dns_servers=[\"1.1.1.1\"])\n",
    "        ips_record = dns_query.dns_lookup(domain)\n",
    "        a = ips_record.response_full[0]\n",
    "        arr = split('\\n', a)\n",
    "        temp_ttls = []\n",
    "        ips.append(ips_record.answer)\n",
    "        num_of_ips.append(len(ips_record.answer))\n",
    "        for entry in arr:\n",
    "            p = entry.split()\n",
    "            temp_ttls.append(int(p[1]))\n",
    "        ttls.append(temp_ttls)\n",
    "        \n",
    "    # Non active malicious domain are not appended\n",
    "    except Exception as exp:\n",
    "        # ttls.append([])\n",
    "        # ips.append([])\n",
    "        # failed_resols += 1\n",
    "        print(domain, 'could not be resolved\\t\\tFailed resolutions', failed_resols)\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "#domains\n",
    "df2['0'] = df_domains['url']\n",
    "#ip\n",
    "df2['ip'] = pd.Series(ips)\n",
    "#ttl\n",
    "df2['ttl'] = pd.Series(ttls)\n",
    "#num_of_ips\n",
    "df2['num_of_ips'] = pd.Series(num_of_ips)\n",
    "\n",
    "\n",
    "#df2['is_active'] = pd.Series(is_active)\n",
    "\n",
    "try:\n",
    "    df2.to_csv(out_csv)\n",
    "except:\n",
    "    df.to_pickle('saved_ttl_df.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of AVG TTL and std TTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ttl = [0 for x in range(6894)]\n",
    "\n",
    "for i in range(5):\n",
    "    # Change path as needed\n",
    "    f_name = f'./newMalWithAds.csv'\n",
    "    df = pd.read_csv(f_name)\n",
    "    ttls = df['ttl']\n",
    "    for index, ttl in enumerate(ttls):\n",
    "        try:\n",
    "            ttl = ttl.replace('\"','')\n",
    "            temp_ttls = ast.literal_eval(ttl) \n",
    "            sum_ttl[index] += temp_ttls[0]\n",
    "        except:\n",
    "            sum_ttl[index] += 0\n",
    "        \n",
    "avg_ttl = [i/5 for i in sum_ttl]\n",
    "\n",
    "sum_ttl = [[] for x in range(6894)]\n",
    "\n",
    "for i in range(5):\n",
    "    # Change path as needed\n",
    "    f_name = f'./newMalWithAds.csv'\n",
    "    df = pd.read_csv(f_name)\n",
    "    ttls = df['ttl']\n",
    "    for index, ttl in enumerate(ttls):\n",
    "        try:\n",
    "            ttl = ttl.replace('\"','')\n",
    "            temp_ttls = ast.literal_eval(ttl) \n",
    "            sum_ttl[index].append(temp_ttls[0])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "avg_ttl = [np.std(i) if np.std(i)!=np.nan else 0 for i in sum_ttl]\n",
    "\n",
    "df_out = pd.DataFrame()\n",
    "\n",
    "df_out['0'] = df['domains']\n",
    "df_out['1'] = pd.Series(avg_ttl)\n",
    "df_out.to_csv('std_ttls.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Data from URLScan and WHOIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "##############################################\n",
    "# Author: Nitay Hason\n",
    "# Append urlscan.io information to the dns dataset\n",
    "##############################################\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/mnt/c/Users/Daniel/Documents/uni/semester_6/detection_of_cyber_attacks/final_project/malicious_URL/robust-malicious-url-detection-master\")\n",
    "\n",
    "from Tools.UrlScan import *\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import whois\n",
    "import tldextract\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-a', '--apikey', help='API key for Urlscan')\n",
    "parser.add_argument('-i', '--csv_in', help='CSV input file datatset')\n",
    "parser.add_argument('-o', '--csv_out', help='CSV output file datatset')\n",
    "parser.add_argument('-d', '--dataframe', help='Dataframe save file')\n",
    "\n",
    "args = parser.parse_args()\n",
    "path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "\n",
    "def checkWhois(domain):\n",
    "    try:\n",
    "        whos = whois.query(domain)\n",
    "        if whos.creation_date is not None and whos.expiration_date is not None and whos.last_updated is not None:\n",
    "            creation_date = whos.creation_date if not isinstance(whos.creation_date, (list,)) else whos.creation_date[0]\n",
    "            expiration_date = whos.expiration_date if not isinstance(whos.expiration_date, (list,)) else \\\n",
    "            whos.expiration_date[0]\n",
    "            last_updated = whos.last_updated if not isinstance(whos.last_updated, (list,)) else whos.last_updated[0]\n",
    "            return [creation_date.timestamp(), expiration_date.timestamp(), last_updated.timestamp()]\n",
    "    except Exception as exp:\n",
    "        print(exp)\n",
    "    return []\n",
    "\n",
    "\n",
    "def fixCountries(asns):\n",
    "    countries = []\n",
    "    asns_countries = {}\n",
    "    with open(os.path.join(path, '../Datasets/asns/asns_countries.json'), 'r') as f:\n",
    "        asns_countries = json.load(f)\n",
    "    for asn in asns:\n",
    "        try:\n",
    "            country = asns_countries[asn]\n",
    "            countries.append(country)\n",
    "        except Exception as exp:\n",
    "            print(exp)\n",
    "    return countries\n",
    "\n",
    "\n",
    "API_KEY = \"6ce22dd8-480b-4007-92ad-3ee9fcfba9fb\"  # args.apikey\n",
    "csv_in = \"./domains.csv\"  # args.csv_in\n",
    "csv_out = \"urlScan_newMAl.csv\"  # args.csv_out\n",
    "dataframe_save = \"df_save\" if args.dataframe is None else args.dataframe\n",
    "\n",
    "domains = []\n",
    "domains_data = {}\n",
    "\n",
    "if os.path.isfile(dataframe_save+'2'):\n",
    "    \n",
    "    df = pd.read_pickle(dataframe_save+'1')\n",
    "    ind = len(df.columns) - 8\n",
    "    print(df[1] != '[]')\n",
    "    last_ind = df[df[1] != '[]'].iloc[-1].name\n",
    "    domains = list(df[0].iloc[:(last_ind + 1)])\n",
    "\n",
    "    for index, row in df.iloc[:(last_ind + 1)].iterrows():\n",
    "        asns = df.at[index, ind]\n",
    "        countries = df.at[index, ind + 1]\n",
    "        ips = df.at[index, ind + 2]\n",
    "        domains_scans = df.at[index, ind + 3]\n",
    "        urls = df.at[index, ind + 4]\n",
    "        servers = df.at[index, ind + 5]\n",
    "        whos = df.at[index, ind + 6]\n",
    "        domains_data[row[0]] = [asns, countries, ips, domains_scans, urls, servers, whos]\n",
    "else:\n",
    "    df = pd.read_csv(csv_in, sep=\";\", header=None, dtype=str)\n",
    "    ind = len(df.columns) - 1\n",
    "    df = df.rename(columns={ind: ind + 7})\n",
    "    df[ind] = '[]'\n",
    "    df[ind + 1] = '[]'\n",
    "    df[ind + 2] = '[]'\n",
    "    df[ind + 3] = '[]'\n",
    "    df[ind + 4] = '[]'\n",
    "    df[ind + 5] = '[]'\n",
    "    df[ind + 6] = '[]'\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    print(index, end=\",\", flush=True)\n",
    "    try:\n",
    "        if row[0] not in domains:\n",
    "            ext = tldextract.extract(row[0])\n",
    "            domain = ext.domain + '.' + ext.suffix\n",
    "            whos = []\n",
    "            u = UrlScan(apikey=API_KEY, url=row[0])\n",
    "            # Starting a scan\n",
    "            try:\n",
    "                u.submit()  # Wait a few seconds for the scan to complete, you can check with u.checkStatus()\n",
    "                result = None\n",
    "                while result is None:\n",
    "                    try:\n",
    "                        result = u.getJson()\n",
    "                        if 'message' in result:\n",
    "                            if result['message'] == 'notdone':\n",
    "                                result = None\n",
    "                                raise Exception('Not done')\n",
    "                    except Exception as exp:\n",
    "                        time.sleep(1)\n",
    "                        #print(exp)\n",
    "                        \n",
    "\n",
    "                if result is not None:\n",
    "                    try:\n",
    "                        result[\"lists\"][\"countries\"] = fixCountries(result[\"lists\"][\"asns\"])\n",
    "                    except:\n",
    "                        pass\n",
    "                if ext.suffix != '':\n",
    "                    whos = checkWhois(domain)\n",
    "                else:\n",
    "                    print(ext)\n",
    "                asns = result[\"lists\"][\"asns\"]\n",
    "                countries = result[\"lists\"][\"countries\"]\n",
    "                ips = result[\"lists\"][\"ips\"]\n",
    "                domains_scans = result[\"lists\"][\"domains\"]\n",
    "                urls = result[\"lists\"][\"urls\"]\n",
    "                servers = result[\"lists\"][\"servers\"]\n",
    "                df.at[index, ind] = str(asns)\n",
    "                df.at[index, ind + 1] = str(countries)\n",
    "                df.at[index, ind + 2] = str(ips)\n",
    "                df.at[index, ind + 3] = str(domains_scans)\n",
    "                df.at[index, ind + 4] = str(urls)\n",
    "                df.at[index, ind + 5] = str(servers)\n",
    "                df.at[index, ind + 6] = str(whos)\n",
    "\n",
    "                domains.append(row[0])\n",
    "                domains_data[row[0]] = [asns, countries, ips, domains_scans, urls, servers, whos]\n",
    "\n",
    "            except Exception as exp:\n",
    "                print(exp)\n",
    "        else:\n",
    "            df.at[index, ind] = str(domains_data[row[0]][0])\n",
    "            df.at[index, ind + 1] = str(domains_data[row[0]][1])\n",
    "            df.at[index, ind + 2] = str(domains_data[row[0]][2])\n",
    "            df.at[index, ind + 3] = str(domains_data[row[0]][3])\n",
    "            df.at[index, ind + 4] = str(domains_data[row[0]][4])\n",
    "            df.at[index, ind + 5] = str(domains_data[row[0]][5])\n",
    "            df.at[index, ind + 6] = str(domains_data[row[0]][6])\n",
    "\n",
    "        df.to_pickle(dataframe_save)\n",
    "        df.to_csv('data_from_whois_urlscan.csv')\n",
    "    except Exception as exp:\n",
    "        print(exp)\n",
    "\n",
    "df.to_csv(csv_out)\n",
    "## If dataframe_save exists, delete it\n",
    "if os.path.isfile(dataframe_save):\n",
    "    os.remove(dataframe_save)\n",
    "else:\n",
    "    print(\"Error: %s file not found\" % dataframe_save)\n",
    "print(\"FINISH\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After collecting Data from URLScan and WhoIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'data_from_whois_urlscan.csv'\n",
    "\n",
    "df_out = pd.DataFrame() \n",
    "\n",
    "def get_max_cons_chars(domain):\n",
    "    max_cons = 1\n",
    "    temp_cons = 1\n",
    "    if domain[:3] == 'www':\n",
    "        domain = domain[4:]\n",
    "    try:\n",
    "        prev_char = domain[0]\n",
    "    except:\n",
    "        return 0    \n",
    "    for c in domain[1:]:\n",
    "        if c == prev_char:\n",
    "            temp_cons += 1\n",
    "        else:\n",
    "            if temp_cons > max_cons:\n",
    "                max_cons = temp_cons\n",
    "            temp_cons = 1\n",
    "        prev_char = c\n",
    "    \n",
    "    return max_cons\n",
    "\n",
    "\n",
    "def number_of_distinct(locations):\n",
    "    a = ast.literal_eval(countries)\n",
    "    return(len(set(a)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    df = pd.read_csv(FILE_NAME)\n",
    "\n",
    "\n",
    "############ LIFE TIME + ACTIVE TIME ###############\n",
    "    \n",
    "    times = df['9']\n",
    "    results = []\n",
    "    results2 = []\n",
    "    #print(times)\n",
    "    for index, time_features in enumerate(times):\n",
    "        if (len(str(time_features).split(', ')) == 3):\n",
    "            creation_date = float(str(time_features).split(', ')[0].split('[')[1])\n",
    "            expiration_date = float(str(time_features).split(', ')[1])\n",
    "            last_update = float(str(time_features).split(', ')[2].split(']')[0])\n",
    "            active_time = last_update - creation_date\n",
    "            life_time = expiration_date - creation_date\n",
    "            results.append(active_time)\n",
    "            results2.append(life_time)\n",
    "        else:\n",
    "            results.append(-1)\n",
    "            results2.append(-1)\n",
    "\n",
    "########### DOMAIN LENGTH + CONSECUTIVE CHARS ###############\n",
    "\n",
    "    domain_length = []\n",
    "    cons_chars = []\n",
    "\n",
    "    urls = df['0']\n",
    "\n",
    "    for index, url in enumerate(urls):\n",
    "        domain = urlparse(url).netloc\n",
    "        domain_length.append(len(domain))\n",
    "        cons_chars.append(get_max_cons_chars(domain))\n",
    "\n",
    "   \n",
    "\n",
    "    df_out['url'] = df['0']\n",
    "    df_out['dom_length'] = pd.Series(domain_length)\n",
    "    df_out['consecutive_chars'] = pd.Series(cons_chars)\n",
    "\n",
    "############ ENTROPY ###############\n",
    "\n",
    "    entr_file = 'CollectEntropy.csv'\n",
    "    df_entr = pd.read_csv(entr_file)\n",
    "    #df_temp= pd.DataFrame(columns=['0'])\n",
    "    #df_temp=df_temp['0'].append('0')\n",
    "    #df_temp =df_temp['0'].append(df_entr['4'])\n",
    "    \n",
    "    a = df_entr['4'].values.tolist()\n",
    "    a.insert(0, 0)\n",
    "    \n",
    "    df_out['entropy'] = pd.Series(a)\n",
    "\n",
    "########### NUM OF IPS ########### \n",
    "\n",
    "    ips = df['1']\n",
    "    num_of_ips = []\n",
    "    for index, ip in enumerate(ips):\n",
    "        ip = ip.replace('b', '')\n",
    "        try:\n",
    "            a = ast.literal_eval(ip)\n",
    "            num_of_ips.append(len(a))\n",
    "        except:\n",
    "            num_of_ips.append(0)\n",
    "    df_out['num_of_ips'] = pd.Series(num_of_ips)\n",
    "    \n",
    "\n",
    "    ############## GEO LOCATIONS ################\n",
    "\n",
    "    locations = df['4']\n",
    "    results3 = []\n",
    "    for index, countries in enumerate(locations):       \n",
    "        if number_of_distinct(countries) == 0:\n",
    "            results3.append(-1)\n",
    "        else:       \n",
    "            results3.append(number_of_distinct(countries))\n",
    "\n",
    "    df_out['distinct_geo_locations'] = pd.Series(results3)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ############# TTL ###############\n",
    "\n",
    "    ttl_file = 'avg_ttls.csv'\n",
    "    df_ttl = pd.read_csv(ttl_file)\n",
    "    ttls = df_ttl['1'].values.tolist()\n",
    "    ttls.insert(0,0)\n",
    "\n",
    "    df_out['avg_ttl'] = pd.Series(ttls)\n",
    "\n",
    "    ttl_std_file = 'std_ttls.csv'\n",
    "    df_ttl = pd.read_csv(ttl_std_file)\n",
    "    std_ttls = df_ttl['1'].values.tolist()\n",
    "    std_ttls.insert(0,0)\n",
    "\n",
    "    df_out['std_ttl'] = pd.Series(std_ttls)\n",
    "\n",
    "    \n",
    "\n",
    "    df_out['life_time'] = pd.Series(results2)\n",
    "    df_out['active_time'] = pd.Series(results)\n",
    "\n",
    "############ PASSIVE DNS ###############\n",
    "\n",
    "    pdns_file = 'LengthOfDomains.csv'\n",
    "    pdns_df = pd.read_csv(pdns_file)\n",
    "    pdns = pdns_df['3'].values.tolist()\n",
    "    pdns.insert(0,0)\n",
    "\n",
    "  \n",
    "    \n",
    "    df_out['passive_dns'] = pd.Series(pdns) \n",
    "\n",
    "\n",
    "     \n",
    " \n",
    "\n",
    "\n",
    "############ SSL Remaining time ###############\n",
    "   \n",
    "    SSL_file = 'CollectSSL.csv'\n",
    "    SSL_df = pd.read_csv(SSL_file)\n",
    "    ssl = SSL_df['6'].values.tolist()\n",
    "    ssl.insert(0,0)\n",
    "    \n",
    "    df_out['ssl_time'] = pd.Series(ssl) \n",
    "\n",
    "\n",
    "    df_out.to_csv('active_time.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "domains_file = './domains.csv'\n",
    "df= pd.read_csv(domains_file)\n",
    "domains = df['domain']\n",
    "df['length']=\"\"\n",
    "df['analytics']=\"\"\n",
    "df['ads']=\"\"\n",
    "counter=0\n",
    "url=\"\"\n",
    "error_counter=0\n",
    "for index, domain in enumerate(domains):\n",
    "    print(f'Run number {index} / {len(domains)}')\n",
    "    if counter%3 == 0:\n",
    "        url = f'https://api.builtwith.com/free1/api.json?KEY=b3051c45-25c1-4ac5-b13a-c840091d0841&LOOKUP={domain}'\n",
    "    elif counter%3 == 1:\n",
    "        url= f'https://api.builtwith.com/free1/api.json?KEY=1a30a8d5-3080-4792-8c0e-730c89436e83&LOOKUP={domain}'\n",
    "    elif counter%3 == 2:\n",
    "        url=f'https://api.builtwith.com/free1/api.json?KEY=c72cbb9a-039b-4e5d-8848-fa90af9b0168&LOOKUP={domain}'\n",
    "\n",
    "   \n",
    "    res = requests.get(url)\n",
    "    ans = res.text\n",
    "    data = ans\n",
    "    time.sleep(0.4)\n",
    "    if ans.find('Errors') == -1:\n",
    "        # error_counter+=1\n",
    "        # print(ans)\n",
    "        # df['builtwith'][index]=\"-1\"\n",
    "        \n",
    "        # print(f'Error number {error_counter}')\n",
    "        counter+=1\n",
    "        df['length'][index]=len(data)\n",
    "        \n",
    "        if data.find('analytics\",\"live\"')!=-1:    \n",
    "            index_data=data.find('analytics\",\"live\"')\n",
    "            temp_data=data[index_data:]\n",
    "            end_index=temp_data.find('\"latest\"')\n",
    "            new_data=temp_data[:end_index]\n",
    "            a = re.split(',|:',new_data)\n",
    "            analytics_number=int(a[2])+int(a[4])\n",
    "            df['analytics'][index]=analytics_number\n",
    "        else:\n",
    "            df['analytics'][index]=0\n",
    "            \n",
    "        if data.find('ads\",\"live\"')!=-1:    \n",
    "            index_data=data.find('ads\",\"live\"')\n",
    "            temp_data=data[index_data:]\n",
    "            end_index=temp_data.find('\"latest\"')\n",
    "            new_data=temp_data[:end_index]\n",
    "            a = re.split(',|:',new_data)\n",
    "            analytics_number=int(a[2])+int(a[4])\n",
    "            df['ads'][index]=analytics_number\n",
    "        else:\n",
    "            df['ads'][index]=0\n",
    "\n",
    "df.to_csv('./newMalWithAds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Builtwith Data Extractor and Feature maker, with Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path as needed\n",
    "\n",
    "\n",
    "domains_file = 'superFinalWithAds.csv'\n",
    "df           = pd.read_csv(domains_file) \n",
    "\n",
    "def statistics(csv_path):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(csv_path)\n",
    "    total = 0\n",
    "    malicous = 0\n",
    "    for index, row in df.iterrows():\n",
    "        total = total + 1\n",
    "        if(row['1'] == 1):\n",
    "            malicous = malicous + 1\n",
    "    print(\"Malicous: \"+str((malicous/total)*100))\n",
    "\n",
    "statistics(\"superFinalWithAds.csv\")\n",
    "\n",
    "#AVG of builtwith length data\n",
    "\n",
    "domains_file='superFinalWithAds.csv'\n",
    "df=pd.read_csv(domains_file)\n",
    "benign_sum=0\n",
    "benign_quantity=0\n",
    "malicious_sum=0\n",
    "malicious_quantity=0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['2'] !=-1:\n",
    "        if row['1'] == 1:\n",
    "            malicious_quantity+=1\n",
    "            malicious_sum+=row['2']\n",
    "        elif row['1'] == 0:\n",
    "            benign_quantity+=1\n",
    "            benign_sum+=row['2']\n",
    "mal_avg=(malicious_sum/malicious_quantity)\n",
    "benign_avg=(benign_sum/benign_quantity)\n",
    "print(f'Malicious AVG length: {mal_avg}')\n",
    "print(f'Benign AVG length: {benign_avg}')\n",
    "\n",
    "\n",
    "#AVG of builtwith analytics data           \n",
    "\n",
    "domains_file='superFinalWithAds.csv'\n",
    "df=pd.read_csv(domains_file)\n",
    "benign_sum=0\n",
    "benign_quantity=0\n",
    "malicious_sum=0\n",
    "malicious_quantity=0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['3'] !=-1:\n",
    "        if row['1'] == 1:\n",
    "            malicious_quantity+=1\n",
    "            malicious_sum+=row['3']\n",
    "        elif row['1'] == 0:\n",
    "            benign_quantity+=1\n",
    "            benign_sum+=row['3']\n",
    "            #print(benign_sum, index)\n",
    "mal_avg=(malicious_sum/malicious_quantity)\n",
    "benign_avg=(benign_sum/benign_quantity)\n",
    "print(f'Malicious AVG analytics: {mal_avg}')\n",
    "print(f'Benign AVG analytics: {benign_avg}')\n",
    "\n",
    "#AVG of builtwith analytics data           \n",
    "\n",
    "domains_file='superFinalWithAds.csv'\n",
    "df=pd.read_csv(domains_file)\n",
    "benign_sum=0\n",
    "benign_quantity=0\n",
    "malicious_sum=0\n",
    "malicious_quantity=0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['17'] !=-1:\n",
    "        if row['1'] == 1:\n",
    "            malicious_quantity+=1\n",
    "            malicious_sum+=row['17']\n",
    "        elif row['1'] == 0:\n",
    "            benign_quantity+=1\n",
    "            benign_sum+=row['17']\n",
    "            #print(benign_sum, index)\n",
    "mal_avg=(malicious_sum/malicious_quantity)\n",
    "benign_avg=(benign_sum/benign_quantity)\n",
    "print(f'Malicious AVG ads: {mal_avg}')\n",
    "print(f'Benign AVG ads: {benign_avg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCR and CAR Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCR(countries):\n",
    "    CCR_NAME = 'countries_(75-25).csv'\n",
    "    countries_ratios = pd.read_csv(CCR_NAME, sep=';')\n",
    "    countries_ratios[\"total_normalize\"] = (countries_ratios[\"total\"] - countries_ratios[\"total\"].min()) / (\n",
    "            countries_ratios[\"total\"].max() - countries_ratios[\"total\"].min())\n",
    "    countries_threshold = 9\n",
    "    rating = 0\n",
    "    neg = 0.00001\n",
    "    for country in countries:\n",
    "        prec = 0.75\n",
    "        calc = 1\n",
    "        cur = countries_ratios[countries_ratios[\"code\"]==country]\n",
    "        if cur.shape[0]>0:\n",
    "            country_total = int(cur[\"total\"].iloc[0])\n",
    "            if country_total>=countries_threshold:\n",
    "                calc = float(cur[\"total_normalize\"])+neg\n",
    "                prec = float(cur[\"benign_ratio\"])\n",
    "                # rating+=math.log(prec+0.00001,0.5)\n",
    "        # else:\n",
    "        # \tprint(country)\n",
    "        rating+=math.log((prec)+neg,0.5)/calc\n",
    "        # print(\"Prec %.5f, Calc %.9f, Total %.5f, Rating %.5f\" % (prec, calc, math.log((prec)+neg,0.05)/(calc+neg), rating))\n",
    "    return rating\n",
    "\n",
    "## Communication ASNs Rank\n",
    "def CAR (asns):\n",
    "    CAR_NAME = 'asns_(75-25).csv'\n",
    "    asns_ratios = pd.read_csv(CAR_NAME, sep=';')\n",
    "    asns_ratios[\"total_normalize\"] = (asns_ratios[\"total\"] - asns_ratios[\"total\"].min()) / (\n",
    "            asns_ratios[\"total\"].max() - asns_ratios[\"total\"].min())\n",
    "    asns_threshold = 2\n",
    "    rating = 0\n",
    "    neg = 0.00001\n",
    "    for asn in asns:\n",
    "        prec = 0.75\n",
    "        calc = 1\n",
    "        cur = asns_ratios[asns_ratios[\"code\"]==asn]\n",
    "        if cur.shape[0]>0:\n",
    "            asn_total = int(cur[\"total\"].iloc[0])\n",
    "            if asn_total>=asns_threshold:\n",
    "                calc = float(cur[\"total_normalize\"])+neg\n",
    "                prec = float(cur[\"benign_ratio\"])\n",
    "        rating+=math.log((prec)+neg,0.5)/calc\n",
    "    return rating\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    df = pd.read_csv('domains2.csv')\n",
    "    country_codes = df['4']\n",
    "    ccr_res = []\n",
    "    '''\n",
    "    for countries in country_codes:\n",
    "        if pd.isnull(countries) == False:\n",
    "            #print(countries, end='\\t')\n",
    "            lst = ast.literal_eval(countries)\n",
    "            ccr_res.append(CCR(lst))\n",
    "        else:\n",
    "            ccr_res.append(-1)\n",
    "    print('CCR feature created')\n",
    "    '''\n",
    "    car_res = []\n",
    "    asns_codes = df['3']\n",
    "    for asns in asns_codes:\n",
    "        if (asns is not np.nan):\n",
    "            lst = ast.literal_eval(asns)\n",
    "            #print(CCR(lst))\n",
    "            car_res.append(CAR(lst))\n",
    "        else:\n",
    "            ccr_res.append(-1)\n",
    "    print('CAR feature created')\n",
    "    #df['10'] = ccr_res\n",
    "    #df['11'] = car_res\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['1']= car_res\n",
    "    #new_df['2']= car_res\n",
    "    #print(f'ccr length {len(ccr_res)} car length {len(car_res)}')\n",
    "    new_df.to_csv('CCR_CAR.csv', index=False)\n",
    "    end = time.time()\n",
    "    print('File saved.')\n",
    "    print('Time:\\t'+str(end-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration- Feature's Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data exploration process we found out that the base features of the authors are not always make effective seperation between malicious and benign URLs. <br>\n",
    "Most of these features are easy to break by adversary in our opinion, even features like number of consecutive chars that authors called them \"base robust\" (we also found out that this feature not making good speration as you can see below). <br>\n",
    "We wanted to find features that will be robust and not easy to break by adversary. These features also need to make pretty good speration between malicious and benign URLs. <br>\n",
    "We do belive in the main idea of the authors \"Less is more, malicious URLs detection with robust features\".\n",
    "Therefore we made exploration process to find features that will be usefull along with their 4 novel features to make good calssify with 6 robust features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        Unnamed: 0                                                  0  1     2  \\\n",
       "0               0                  https://gestaumdigital.com.br/WP/  1  3452   \n",
       "1               1                https://smsorg6.wixsite.com/serveur  1  5797   \n",
       "2               2      http://ip-184-168-122-113.ip.secureserver.net  1  4534   \n",
       "3               3                   https://167-99-65-41.cprapid.com  1  1306   \n",
       "4               4  http://a0545169.xsph.ru/postale/authentificati...  1  6888   \n",
       "...           ...                                                ... ..   ...   \n",
       "29456       29456                                        navi-cs.com  1  1945   \n",
       "29457       29457                           mehannoceo.ihostfull.com  1  5975   \n",
       "29458       29458                                    ewaveleague.com  1     0   \n",
       "29459       29459                             glamorousworldhair.com  1     0   \n",
       "29460       29460                                      cncselect.com  1  3517   \n",
       "\n",
       "        3   4  5          6  7  8            9           10  11  12   13  \\\n",
       "0       5  21  1   5.759356  2  1   300.000000  1283.327925   1   0   69   \n",
       "1       7  19  1   5.785102  3  2   300.000000  1283.327925  10   9    4   \n",
       "2       6  38  2  10.293215  1  1  3600.000000  1283.327925  23  22    6   \n",
       "3       0  24  2   6.429323  1  1  3600.000000  1283.327925   4   1    5   \n",
       "4      14  16  1   4.500000  1  1  7200.000000  1283.327925   8   6   24   \n",
       "...    ..  .. ..        ... .. ..          ...          ...  ..  ..  ...   \n",
       "29456   5  11  1   3.724783  0  1    34.544186  1131.608809   8   6  103   \n",
       "29457   7  24  2   7.052005  1  1   299.000000  1507.178242   9   3    5   \n",
       "29458   0  15  1   5.153322  2  1   300.000000     0.000000   6   4   40   \n",
       "29459   0  22  1   6.675872  1  1  4285.630303   637.837468   9   7   38   \n",
       "29460  10  13  1   4.916298  1  1   300.000000    96.129875  10   9   60   \n",
       "\n",
       "            14          15           16  17  \n",
       "0            0    1.655011     2.436508   0  \n",
       "1            0    0.334583    43.382116   5  \n",
       "2            0    1.505625     8.019406   5  \n",
       "3            0    3.973535  5500.256959   0  \n",
       "4            0  160.879284     0.587004  12  \n",
       "...        ...         ...          ...  ..  \n",
       "29456        0    0.162200     2.077890   0  \n",
       "29457        0    0.000000     0.000000   4  \n",
       "29458        0    7.950809     0.984370   0  \n",
       "29459  2332800    0.000000     0.000000   0  \n",
       "29460        0  157.549259     0.415018  12  \n",
       "\n",
       "[29461 rows x 19 columns]>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('superFinalWithAds.csv')\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain length avg benign: 14.504337050805452, malicious: 21.18769551616267\n",
      "Consecutive chars avg benign: 1.282252512735784, malicious: 1.4107142857142858\n",
      "Entropy avg benign: 5.032141735614089, malicious: 6.294115352699701\n",
      "Number of IPs avg benign: 3.2559548395979623, malicious: 1.408628779979145\n",
      "Number of distinct geo locations avg benign: 1.0381844049749873, malicious: 1.3477580813347236\n",
      "TTL avg benign: 6631.092866181676, malicious: 9513.636278776956\n",
      "Std TTL avg benign: 5212.624206123971, malicious: 1590.486815432426\n",
      "Domain life time avg benign: 11.128321630180366, malicious: 8.231360792492179\n",
      "Domain active time avg benign: 8.424709715911698, malicious: 6.161105318039625\n",
      "Number of Passive DNS records avg benign: 40.54844187434026, malicious: 107.61347933776561\n",
      "SSL certificate time avg benign: 12566207.379228052, malicious: 1396974963.8939\n",
      "CCR avg benign: 33.19149363530285, malicious: 140.58508488888222\n",
      "CAR avg benign: 612.468505217337, malicious: 2585.3218840165446\n",
      "ads avg benign: 31.860296479875167, malicious: 10.397549530761209\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def stats(df,label_index, col_index):\n",
    "    benign_sum=0\n",
    "    benign_quantity=0\n",
    "    malicious_sum=0\n",
    "    malicious_quantity=0\n",
    "    for index, row in df.iterrows():\n",
    "        if row[col_index] !=-1:\n",
    "            if row[label_index] == 1:\n",
    "                malicious_quantity+=1\n",
    "                malicious_sum+=row[col_index]\n",
    "            elif row[label_index] == 0:\n",
    "                benign_quantity+=1\n",
    "                benign_sum+=row[col_index]\n",
    "    mal_avg=(malicious_sum/malicious_quantity)\n",
    "    benign_avg=(benign_sum/benign_quantity)\n",
    "    return benign_avg, mal_avg\n",
    "\n",
    "\n",
    "df = pd.read_csv('superFinalWithAds.csv')\n",
    "dom_len_avg_ben, dom_len_avg_mal        = stats(df, '1', '4')\n",
    "con_chars_avg_ben, con_chars_avg_mal    = stats(df, '1','5')\n",
    "entropy_avg_ben, entropy_avg_mal        = stats(df, '1', '6')\n",
    "num_ips_avg_ben, num_ips_avg_mal        = stats(df, '1', '7')\n",
    "dist_geo_avg_ben, dist_geo_avg_mal      = stats(df, '1', '8')\n",
    "ttl_avg_ben, ttl_avg_mal                = stats(df,'1','9')\n",
    "stdttl_avg_ben, stdttl_avg_mal          = stats(df,'1','10')\n",
    "lifetime_avg_ben, lifetime_avg_mal      = stats(df, '1', '11')\n",
    "activetime_avg_ben, activetime_avg_mal  = stats(df, '1', '12')\n",
    "pdns_avg_ben, pdns_avg_mal              = stats(df, '1','13')\n",
    "ssltime_avg_ben, ssltime_avg_mal        = stats(df, '1', '14')\n",
    "ccr_avg_ben, ccr_avg_mal                = stats(df, '1', '15')\n",
    "car_avg_ben, car_avg_mal                = stats(df, '1', '16')\n",
    "ads_avg_ben, ads_avg_mal                = stats(df, '1', '17')\n",
    "print(f'Domain length avg benign: {dom_len_avg_ben}, malicious: {dom_len_avg_mal}')\n",
    "print(f'Consecutive chars avg benign: {con_chars_avg_ben}, malicious: {con_chars_avg_mal}')\n",
    "print(f'Entropy avg benign: {entropy_avg_ben}, malicious: {entropy_avg_mal}')\n",
    "print(f'Number of IPs avg benign: {num_ips_avg_ben}, malicious: {num_ips_avg_mal}')\n",
    "print(f'Number of distinct geo locations avg benign: {dist_geo_avg_ben}, malicious: {dist_geo_avg_mal}')\n",
    "print(f'TTL avg benign: {ttl_avg_ben}, malicious: {ttl_avg_mal}')\n",
    "print(f'Std TTL avg benign: {stdttl_avg_ben}, malicious: {stdttl_avg_mal}')\n",
    "print(f'Domain life time avg benign: {lifetime_avg_ben}, malicious: {lifetime_avg_mal}')\n",
    "print(f'Domain active time avg benign: {activetime_avg_ben}, malicious: {activetime_avg_mal}')\n",
    "print(f'Number of Passive DNS records avg benign: {pdns_avg_ben}, malicious: {pdns_avg_mal}')\n",
    "print(f'SSL certificate time avg benign: {ssltime_avg_ben}, malicious: {ssltime_avg_mal}')\n",
    "print(f'CCR avg benign: {ccr_avg_ben}, malicious: {ccr_avg_mal}')\n",
    "print(f'CAR avg benign: {car_avg_ben}, malicious: {car_avg_mal}')\n",
    "print(f'ads avg benign: {ads_avg_ben}, malicious: {ads_avg_mal}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    x = [1,4,7,10,13]#,16,19,22,25,28,31,34,37]\n",
    "    y = [dom_len_avg_ben, con_chars_avg_ben, entropy_avg_ben, num_ips_avg_ben, dist_geo_avg_ben]#, ttl_avg_ben, stdttl_avg_ben,lifetime_avg_ben,activetime_avg_ben, pdns_avg_ben, ssltime_avg_ben, ccr_avg_ben, car_avg_ben] \n",
    "      \n",
    "    x2 = [2,5,8,11,14]#,17,20,23,26,29,32,35,38] \n",
    "    y2 = [dom_len_avg_mal, con_chars_avg_mal, entropy_avg_mal, num_ips_avg_mal, dist_geo_avg_mal]#, ttl_avg_mal, stdttl_avg_mal,lifetime_avg_mal,activetime_avg_mal, pdns_avg_mal, ssltime_avg_mal, ccr_avg_mal, car_avg_mal] \n",
    "    plt.bar(x, y, color='b', align = 'center') \n",
    "    plt.bar(x2, y2, color = 'r', align = 'center') \n",
    "    plt.title('AVG of old features') \n",
    "    plt.ylabel('Y axis') \n",
    "    plt.xlabel('X axis')  \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    x = [16,19,22,25,28]\n",
    "    y = [ttl_avg_ben, stdttl_avg_ben,(ssltime_avg_ben/10000), ccr_avg_ben, car_avg_ben] \n",
    "      \n",
    "    x2 = [17,20,23,26,29] \n",
    "    y2 = [ttl_avg_mal, stdttl_avg_mal, (ssltime_avg_mal/10000), ccr_avg_mal, car_avg_mal] \n",
    "    plt.bar(x, y, color='b', align = 'center') \n",
    "    plt.bar(x2, y2, color = 'r', align = 'center') \n",
    "    plt.title('AVG of old features') \n",
    "    plt.ylabel('Y axis') \n",
    "    plt.xlabel('X axis')  \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    x = [1,4,7]\n",
    "    y = [lifetime_avg_ben,activetime_avg_ben, pdns_avg_ben]\n",
    "      \n",
    "    x2 = [2,5,8]\n",
    "    y2 = [lifetime_avg_mal,activetime_avg_mal, pdns_avg_mal] \n",
    "    plt.bar(x, y, color='b', align = 'center') \n",
    "    plt.bar(x2, y2, color = 'r', align = 'center') \n",
    "    plt.title('AVG of old features') \n",
    "    plt.ylabel('Y axis') \n",
    "    plt.xlabel('X axis')  \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Checking features - all\tModel: RandomForest\n",
      "[[5321 41]\n",
      " [73 1931]]\n",
      "Accuracy: 0.9845234862883518\n",
      "f1 Score: 0.9803646161087014\n",
      "precision: 0.9828376845732731\n",
      "recall: 0.9779632268473124\n",
      "\n",
      "\n",
      "Checking features - all_new\tModel: RandomForest\n",
      "[[5325 37]\n",
      " [59 1945]]\n",
      "Accuracy: 0.9869671463480858\n",
      "f1 Score: 0.9834910741475679\n",
      "precision: 0.9851867963229242\n",
      "recall: 0.9818292359704315\n"
     ]
    }
   ],
   "source": [
    "features_check = {\n",
    "    \"all\": {\n",
    "\t\t\"features\"     : [4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "        \"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "    \"all_new\": {\n",
    "\t\t\"features\"     : [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "        \"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t}\n",
    "}\n",
    "\n",
    "features_to_check = [\"all\", \"all_new\"]\n",
    "\n",
    "label = 1 # cell one tells if malicious/benign\n",
    "path               = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "features_file_name = \"../Data_fixed_2.csv\"\n",
    "features_file      = os.path.join(path, features_file_name)\n",
    "\n",
    "#for every variable in freatures_to_check (We have 5) we will check if we can find a malicious sign. if so we will append the url to df.\n",
    "for features_set in features_to_check:\n",
    "    print(\"\\n\\nChecking features - %s\" % (features_set), end='\\t')\n",
    "    print(\"Model: RandomForest\")\n",
    "    features_file = os.path.join(path, features_check[features_set][\"feature_file\"])\n",
    "    data = pd.read_csv(features_file)\n",
    "    data.head()\n",
    "    feature_cols = features_check[features_set][\"features\"]\n",
    "    a = [\"0\"] * len(feature_cols)\n",
    "    c = 0\n",
    "    for f in feature_cols:\n",
    "        a[c] = str(f)\n",
    "        c = c+1\n",
    "    \n",
    "    X = data[a] # Features\n",
    "    #print(np.where(np.isnan(X)))\n",
    "    y = data[f'{label}'] # Target variable\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1) # 75% training and 25% test\n",
    "\n",
    "    # Create Decision Tree classifer object\n",
    "    clf = DecisionTreeClassifier()\n",
    "\n",
    "    sc_x = StandardScaler()\n",
    "    X_train = sc_x.fit_transform(X_train)\n",
    "    X_test = sc_x.transform(X_test)\n",
    "\n",
    "    mm_x = MinMaxScaler()\n",
    "    X_train = mm_x.fit_transform(X_train)\n",
    "    X_test = mm_x.transform(X_test)\n",
    "\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    print(f'[[{tn} {fp}]')\n",
    "    print(f' [{fn} {tp}]]')\n",
    "\n",
    "\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
    "    print(\"f1 Score:\",f1_score(y_test, y_pred, average='macro'))\n",
    "    print(\"precision:\", precision_score(y_test,y_pred, average='macro'))\n",
    "    print(\"recall:\", recall_score(y_test,y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Checking features - all_new\tModel: SVM\n",
      "[[5294 68]\n",
      " [169 1835]]\n",
      "Accuracy: 0.9678251425468368\n",
      "f1 Score: 0.9587229411768183\n",
      "precision: 0.9666657817184905\n",
      "recall: 0.9514934137692538\n",
      "\n",
      "\n",
      "Checking features - all\tModel: SVM\n",
      "[[5276 86]\n",
      " [221 1783]]\n",
      "Accuracy: 0.958322020092316\n",
      "f1 Score: 0.9462309009076679\n",
      "precision: 0.956891170659457\n",
      "recall: 0.9368408836932625\n"
     ]
    }
   ],
   "source": [
    "features_check = {\n",
    "    \n",
    "    \"all\": {\n",
    "\t\t\"features\"     : [4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "        \"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "    \"all_new\": {\n",
    "\t\t\"features\"     : [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "        \"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t}\n",
    "}\n",
    "\n",
    "\n",
    "features_to_check = [\"all_new\", \"all\"]\n",
    "label = 1\n",
    "#path = os.path.dirname(os.path.abspath(__file__))\n",
    "#features_file_name = \"../Data_fixed_2.csv\"\n",
    "#features_file = os.path.join(path, features_file_name)\n",
    "features_file = 'superFinalWithAds.csv'\n",
    "# for every variable in freatures_to_check (We have 5) we will check if we can find a malicious sign. if so we will append the url to df.\n",
    "for features_set in features_to_check:\n",
    "    print(\"\\n\\nChecking features - %s\" % (features_set), end='\\t')\n",
    "    print(\"Model: SVM\")\n",
    "    features_file = os.path.join(\n",
    "        path, features_check[features_set][\"feature_file\"])\n",
    "    data = pd.read_csv(features_file)\n",
    "    data.head()\n",
    "    feature_cols = features_check[features_set][\"features\"]\n",
    "    a = [\"0\"] * len(feature_cols)\n",
    "    c = 0\n",
    "    for f in feature_cols:\n",
    "        a[c] = str(f)\n",
    "        c = c+1\n",
    "    # print(a)\n",
    "    X = data[a]  # Features\n",
    "    #print(np.where(np.isnan(X)))\n",
    "    y = data[f'{label}']  # Target variable\n",
    "    # Split dataset into training set and test set\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=1)  # 70% training and 30% test\n",
    "\n",
    "    sc_x = StandardScaler()\n",
    "    X_train = sc_x.fit_transform(X_train)\n",
    "    X_test = sc_x.transform(X_test)\n",
    "\n",
    "    mm_x = MinMaxScaler()\n",
    "    X_train = mm_x.fit_transform(X_train)\n",
    "    X_test = mm_x.transform(X_test)\n",
    "\n",
    "    clf = SVC()\n",
    "\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    print(f'[[{tn} {fp}]')\n",
    "    print(f' [{fn} {tp}]]')\n",
    "\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"f1 Score:\", f1_score(y_test, y_pred,\n",
    "                                        average='macro', labels=np.unique(y_pred)))\n",
    "    print(\"precision:\", precision_score(\n",
    "        y_test, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "    print(\"recall:\", recall_score(y_test, y_pred,\n",
    "                                          average='macro', labels=np.unique(y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Checking features - all_new\n",
      "[[5213 149]\n",
      " [396 1608]]\n",
      "Accuracy: 0.9260114037469455\n",
      "f1 Score: 0.9027076556354089\n",
      "precision: 0.9222977686584483\n",
      "recall: 0.8873035354133211\n",
      "\n",
      "\n",
      "Checking features - all\n",
      "[[5182 180]\n",
      " [468 1536]]\n",
      "Accuracy: 0.9120282378495791\n",
      "f1 Score: 0.8834807775681661\n",
      "precision: 0.9061365183489077\n",
      "recall: 0.8664487511362952\n"
     ]
    }
   ],
   "source": [
    "features_check = {\n",
    "\t\n",
    "    \"all\": {\n",
    "\t\t\"features\"     : [4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "        \"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "    \"all_new\": {\n",
    "\t\t\"features\"     : [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "        \"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t}\n",
    "}\n",
    "\n",
    "features_to_check = [\"all_new\", \"all\"]\n",
    "\n",
    "label = 1\n",
    "path               = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "features_file_name = \"superFinalWithAds.csv\"\n",
    "features_file      = os.path.join(path, features_file_name)\n",
    "\n",
    "\n",
    "#for every variable in freatures_to_check (We have 5) we will check if we can find a malicious sign. if so we will append the url to df.\n",
    "for features_set in features_to_check:\n",
    "    print(\"\\n\\nChecking features - %s\" % (features_set))\n",
    "    features_file = os.path.join(path, features_check[features_set][\"feature_file\"])\n",
    "    data = pd.read_csv(features_file)\n",
    "    data.head()\n",
    "    feature_cols = features_check[features_set][\"features\"]\n",
    "    a = [\"0\"] * len(feature_cols)\n",
    "    c = 0\n",
    "    for f in feature_cols:\n",
    "        a[c] = str(f)\n",
    "        c = c+1\n",
    "    #print(a)\n",
    "    X = data[a] # Features\n",
    "    y = data[f'{label}'] # Target variable\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1) # 75% training and 25% test\n",
    "    # print(X_train)\n",
    "    # print(y_train)\n",
    "    clf = LogisticRegression(solver=\"saga\")\n",
    "\n",
    "    sc_x = StandardScaler()\n",
    "    X_train = sc_x.fit_transform(X_train)\n",
    "    X_test = sc_x.transform(X_test)\n",
    "\n",
    "    mm_x = MinMaxScaler()\n",
    "    X_train = mm_x.fit_transform(X_train)\n",
    "    X_test = mm_x.transform(X_test)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    print(f'[[{tn} {fp}]')\n",
    "    print(f' [{fn} {tp}]]')\n",
    "\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"f1 Score:\", f1_score(y_test, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "    print(\"precision:\", precision_score(y_test, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "    print(\"recall:\", recall_score(y_test, y_pred, average='macro', labels=np.unique(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Checking features - all_new\tModel: KNN\n",
      "[[5280 82]\n",
      " [137 1867]]\n",
      "Accuracy: 0.9702688026065708\n",
      "f1 Score: 0.9621408775488838\n",
      "precision: 0.9663181953928934\n",
      "recall: 0.9581719626766609\n",
      "\n",
      "\n",
      "Checking features - all\tModel: KNN\n",
      "[[5273 89]\n",
      " [144 1860]]\n",
      "Accuracy: 0.9683681781156666\n",
      "f1 Score: 0.9597206596752965\n",
      "precision: 0.963876288593394\n",
      "recall: 0.9557727141762726\n"
     ]
    }
   ],
   "source": [
    "features_check = {\t\n",
    "    \"all\": {\n",
    "\t\t\"features\"     : [4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "        \"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "    \"all_new\": {\n",
    "\t\t\"features\"     : [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "        \"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t}\n",
    "}\n",
    "\n",
    "features_to_check = [\"all_new\", \"all\"]\n",
    "label = 1\n",
    "\n",
    "path               = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "features_file_name = \"../Data_fixed_2.csv\"\n",
    "features_file      = os.path.join(path, features_file_name)\n",
    "\n",
    "#for every variable in freatures_to_check (We have 5) we will check if we can find a malicious sign. if so we will append the url to df.\n",
    "for features_set in features_to_check:\n",
    "    print(\"\\n\\nChecking features - %s\" % (features_set), end='\\t')\n",
    "    print(\"Model: KNN\")\n",
    "    features_file = os.path.join(path, features_check[features_set][\"feature_file\"])\n",
    "    data = pd.read_csv(features_file)\n",
    "    data.head()\n",
    "    feature_cols = features_check[features_set][\"features\"]\n",
    "    a = [\"0\"] * len(feature_cols)\n",
    "    c = 0\n",
    "    for f in feature_cols:\n",
    "        a[c] = str(f)\n",
    "        c = c+1\n",
    "    #print(a)\n",
    "    X = data[a] # Features\n",
    "    y = data[f'{label}'] # Target variable\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)  # 75% training and 25% test\n",
    "    # Create Decision Tree classifer object\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    sc_x = StandardScaler()\n",
    "    X_train = sc_x.fit_transform(X_train)\n",
    "    X_test = sc_x.transform(X_test)\n",
    "\n",
    "    mm_x = MinMaxScaler()\n",
    "    X_train = mm_x.fit_transform(X_train)\n",
    "    X_test = mm_x.transform(X_test)\n",
    "\n",
    "    # Train Decision Tree Classifer\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the response for test dataset\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    print(f'[[{tn} {fp}]')\n",
    "    print(f' [{fn} {tp}]]')\n",
    "\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"f1 Score:\", f1_score(y_test, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "    print(\"precision:\", precision_score(y_test, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "    print(\"recall:\", recall_score(y_test, y_pred, average='macro', labels=np.unique(y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Checking features - all \tModel: Naive Bayes\n",
      "[[5344 18]\n",
      " [1779 225]]\n",
      "Accuracy: 0.756041270703231\n",
      "f1 Score: 0.5281671517169068\n",
      "precision: 0.8380858044623312\n",
      "recall: 0.5544592463711145\n",
      "\n",
      "\n",
      "Checking features - all_new \tModel: Naive Bayes\n",
      "[[5142 220]\n",
      " [723 1281]]\n",
      "Accuracy: 0.8719793646483844\n",
      "f1 Score: 0.8234809171440309\n",
      "precision: 0.8650786943401756\n",
      "recall: 0.7990960451346467\n"
     ]
    }
   ],
   "source": [
    "features_check = {\n",
    "\t\"all\": {\n",
    "\t\t\"features\"     : [4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "        \"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "    \"all_new\": {\n",
    "\t\t\"features\"     : [2,3,4,5,6,7,8,9,10,11,12,13,15,16,17],\n",
    "        \"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t}\n",
    "}\n",
    "\n",
    "features_to_check = [\"all\", \"all_new\"]\n",
    "\n",
    "label = 1\n",
    "\n",
    "path               = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "features_file_name = \"../superFinal.csv\"\n",
    "features_file      = os.path.join(path, features_file_name)\n",
    "\n",
    "#for every variable in freatures_to_check (We have 5) we will check if we can find a malicious sign. if so we will append the url to df.\n",
    "for features_set in features_to_check:\n",
    "    print(\"\\n\\nChecking features - %s\" % (features_set), '\\tModel: Naive Bayes')\n",
    "    features_file = os.path.join(path, features_check[features_set][\"feature_file\"])\n",
    "    data = pd.read_csv(features_file)\n",
    "    data.head()\n",
    "    feature_cols = features_check[features_set][\"features\"]\n",
    "    a = [\"0\"] * len(feature_cols)\n",
    "    c = 0\n",
    "    for f in feature_cols:\n",
    "        a[c] = str(f)\n",
    "        c = c+1\n",
    "    #print(a)\n",
    "    X = data[a] # Features\n",
    "    y = data[f'{label}'] # Target variable\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1) # 70% training and 30% test\n",
    "\n",
    "    # Create Decision Tree classifer object\n",
    "    clf = GaussianNB()\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    print(f'[[{tn} {fp}]')\n",
    "    print(f' [{fn} {tp}]]')\n",
    "\n",
    "\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
    "    print(\"f1 Score:\",f1_score(y_test, y_pred, average='macro'))\n",
    "    print(\"precision:\", precision_score(y_test,y_pred, average='macro'))\n",
    "    print(\"recall:\", recall_score(y_test,y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Author: Nitay Hason\n",
    "# Artificial Neural Network class\n",
    "##############################################\n",
    "class NeuralNetwork:\n",
    "\tdef __init__(self, learning_rate = 0.01, training_epochs = 10000, kfolds=10, batch_size = 150, threshold = 0.5, test_size = 0.25, degree = 1, drop_columns = [], dataset_path=None, dataset=None):\n",
    "\t\t# Hyperparameters\n",
    "\t\tself.dataset\t\t = dataset\n",
    "\t\tself.dataset_path    = dataset_path\n",
    "\t\tself.learning_rate   = learning_rate\n",
    "\t\tself.training_epochs = training_epochs\n",
    "\t\tself.kfolds          = kfolds\n",
    "\t\tself.batch_size      = batch_size\n",
    "\t\tself.threshold       = threshold\n",
    "\t\tself.test_size       = test_size\n",
    "\t\tself.degree          = degree\n",
    "\t\tself.drop_columns    = drop_columns\n",
    "\t\tself.X\t             = None\n",
    "\t\tself.y               = None\n",
    "\t\tself.X_train         = None\n",
    "\t\tself.X_test          = None\n",
    "\t\tself.y_train         = None\n",
    "\t\tself.y_test          = None\n",
    "\t\tself.idx_train       = None\n",
    "\t\tself.idx_test        = None\n",
    "\t\tself.model \t\t\t = None\n",
    "\t\tself.model_history\t = None\n",
    "\t\tself.scaler    \t\t = None\n",
    "\n",
    "\t\"\"\"get dataset from a path and set the dtype\"\"\"\n",
    "\tdef set_dataset(self, dataset_path=None, delimiter=',', skip_header=0, usecols=None, dtype=None):\n",
    "\t\tif dataset_path is not None:\n",
    "\t\t\tself.dataset_path = dataset_path\n",
    "\t\tif self.dataset_path is not None:\n",
    "\t\t\tif dtype is None:\n",
    "\t\t\t\tdtype = np.dtype(np.float64)\n",
    "\t\t\tself.dataset = np.genfromtxt(dataset_path, delimiter=delimiter, skip_header=skip_header, usecols=usecols, dtype=dtype)\n",
    "\n",
    "\t\"\"\"build the dataset without unwanted columns, normalize them and add the features and labels to it\"\"\"\n",
    "\tdef build(self, verbose=1):\n",
    "\t\tuse_dataset = self.dataset.copy()\n",
    "\t\t# Drop unwanted columns\n",
    "\t\tif len(self.drop_columns)>0:\n",
    "\t\t\tuse_dataset = np.delete(self.dataset, self.drop_columns, 1)\n",
    "\t\tuse_dataset = np.asfarray(use_dataset,np.dtype(np.float64))\n",
    "\n",
    "\t\t# Normlize the dataset\n",
    "\t\tself.scaler= MinMaxScaler().fit(use_dataset[:, :-1])\n",
    "\t\tdataset_norm = self.scaler.transform(use_dataset[:, :-1])\n",
    "\n",
    "\t\t# Split into features and labels\n",
    "\t\tself.X, self.y = dataset_norm, np.transpose([use_dataset[:, -1]]).ravel()\n",
    "\n",
    "\t\tif self.degree > 1:\n",
    "\t\t\tpolynomial_features= PolynomialFeatures(degree=self.degree, include_bias=False)\n",
    "\t\t\tself.X = polynomial_features.fit_transform(self.X)\n",
    "\n",
    "\t\"\"\"classify which layer it is and append to it the proper properties\"\"\"\n",
    "\tdef get_layer(self, layer, input_dim=0):\n",
    "\t\tret_layer = []\n",
    "\t\tif layer[1] == \"relu\":\n",
    "\t\t\tif input_dim>0:\n",
    "\t\t\t\tret_layer.append(tf.keras.layers.Dense(units=layer[0], input_dim=input_dim))\n",
    "\t\t\t\tret_layer.append(tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "\t\t\telse:\n",
    "\t\t\t\tret_layer.append(tf.keras.layers.Dense(units=layer[0]))\n",
    "\t\t\t\tret_layer.append(tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "\t\telif layer[1] == \"leakyrelu\":\n",
    "\t\t\tif input_dim>0:\n",
    "\t\t\t\tret_layer.append(tf.keras.layers.Dense(units=layer[0], input_dim=input_dim))\n",
    "\t\t\t\tret_layer.append(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "\t\t\telse:\n",
    "\t\t\t\tret_layer.append(tf.keras.layers.Dense(units=layer[0]))\n",
    "\t\t\t\tret_layer.append(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "\t\telif layer[1] == \"sigmoid\":\n",
    "\t\t\tret_layer.append(tf.keras.layers.Dense(units=layer[0], activation='sigmoid'))\n",
    "\t\treturn ret_layer\n",
    "\n",
    "\n",
    "\t\"\"\"train the data according to the logistic regression properties. and print the training time. \"\"\"\n",
    "\tdef train(self, optimizer=0, layers=[(80,\"relu\"),(80,\"relu\"),(80,\"leakyrelu\"),(1,'sigmoid')], verbose=0):\n",
    "\t\t# Split the data to train and test\n",
    "\t\tindices = np.arange(self.y.shape[0])\n",
    "\t\tself.X_train, self.X_test, self.y_train, self.y_test, self.idx_train, self.idx_test = train_test_split(self.X, self.y, indices, stratify=self.y, test_size=self.test_size, random_state=42)\n",
    "\n",
    "\t\t# sc_x = StandardScaler()\n",
    "\t\t# self.X_train = sc_x.fit_transform(self.X_train)\n",
    "\t\t# self.X_test = sc_x.transform(self.X_test)\n",
    "\t\t#\n",
    "\t\t# mm_x = MinMaxScaler()\n",
    "\t\t# self.X_train = mm_x.fit_transform(self.X_train)\n",
    "\t\t# self.X_test = mm_x.transform(self.X_test)\n",
    "\n",
    "\t\tinput_dim  = self.X_train.shape[1]\n",
    "\t\tself.model = tf.keras.models.Sequential()\n",
    "\n",
    "\t\ti = 0\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tif i==0:\n",
    "\t\t\t\ti = 1\n",
    "\t\t\t\tret_layer = self.get_layer(layer, input_dim=input_dim)\n",
    "\t\t\telse:\n",
    "\t\t\t\tret_layer = self.get_layer(layer)\n",
    "\n",
    "\t\t\tfor add in ret_layer:\n",
    "\t\t\t\t\tself.model.add(add)\n",
    "\n",
    "\n",
    "\t\tif optimizer == 0:\n",
    "\t\t\toptimizer = tf.keras.optimizers.Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\t\telse:\n",
    "\t\t\toptimizer = tf.keras.optimizers.SGD(lr=self.learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "\t\tself.model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\t\tcallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=0, mode='auto')\n",
    "\n",
    "\t\tkf = KFold(n_splits=self.kfolds, random_state=None, shuffle=False)\n",
    "\t\tkf.get_n_splits(self.X)\n",
    "\n",
    "\t\tprint(\"Start training\")\n",
    "\t\tstart   = time.time()\n",
    "\t\tfor train_index, test_index in kf.split(self.idx_train):\n",
    "\t\t\tX_train_fold, X_test_fold = self.X_train[train_index], self.X_train[test_index]\n",
    "\t\t\ty_train_fold, y_test_fold = self.y_train[train_index], self.y_train[test_index]\n",
    "\n",
    "\t\t\tself.model_history = self.model.fit(X_train_fold, y_train_fold, epochs=self.training_epochs, batch_size=self.batch_size, validation_data=(X_test_fold,y_test_fold), callbacks=[callback], verbose=verbose)\n",
    "\t\t# self.model_history = self.model.fit(self.X_train, self.y_train, epochs=self.training_epochs, batch_size=self.batch_size, validation_split=self.test_size, callbacks=[callback], verbose=verbose)\n",
    "\t\tend = time.time()\n",
    "\t\tprint(\"\\nTraining time:\")\n",
    "\t\tprint(end - start)\n",
    "\n",
    "\tdef predict(self, verbose=1):\n",
    "\t\t# predict probabilities for test set\n",
    "\t\tyhat_probs = self.model.predict(self.X_test, verbose=1)\n",
    "\t\treturn self.predict_check(yhat_probs, verbose=verbose)\n",
    "\n",
    "\n",
    "\tdef predict_self(self, X, threshold=None, verbose=1):\n",
    "\t\tif threshold is None:\n",
    "\t\t\tthreshold = self.threshold\n",
    "\t\tX_norm = self.scaler.transform(X)\n",
    "\t\tif self.degree > 1:\n",
    "\t\t\tpolynomial_features= PolynomialFeatures(degree=self.degree, include_bias=False)\n",
    "\t\t\tX_norm = polynomial_features.fit_transform(X_norm)\n",
    "\n",
    "\t\tyhat_probs = self.model.predict(X_norm, verbose=0)\n",
    "\t\tyhat_classes = yhat_probs.copy()\n",
    "\t\tyhat_classes[yhat_classes>=threshold]=np.float64(1)\n",
    "\t\tyhat_classes[yhat_classes<threshold]=np.float64(0)\n",
    "\t\treturn [yhat_probs,yhat_classes]\n",
    "\n",
    "\tdef predict_check(self, predicted_probs, verbose=1):\n",
    "\t\t# reduce to 1d array\n",
    "\t\tyhat_probs = predicted_probs[:, 0]\n",
    "\t\tyhat_classes = yhat_probs.copy()\n",
    "\t\tyhat_classes[yhat_classes>=self.threshold]=np.float64(1)\n",
    "\t\tyhat_classes[yhat_classes<self.threshold]=np.float64(0)\n",
    "\n",
    "\t\tFPR, TPR, _ = roc_curve(self.y_test, yhat_probs)\n",
    "\t\tAUC = auc(FPR, TPR)\n",
    "\n",
    "\t\t# accuracy: (tp + tn) / (p + n)\n",
    "\t\taccuracy    = accuracy_score(self.y_test, yhat_classes)\n",
    "\t\t# precision tp / (tp + fp)\n",
    "\t\tprecision   = precision_score(self.y_test, yhat_classes)\n",
    "\t\t# recall: tp / (tp + fn)\n",
    "\t\trecall      = recall_score(self.y_test, yhat_classes)\n",
    "\t\t# f1: 2 tp / (2 tp + fp + fn)\n",
    "\t\tf1          = f1_score(self.y_test, yhat_classes)\n",
    "\t\t#loss\n",
    "\t\tloss        = log_loss(self.y_test, yhat_classes, eps=1e-7)\n",
    "\t\t# kappa\n",
    "\t\tkappa       = cohen_kappa_score(self.y_test, yhat_classes)\n",
    "\t\t# ROC AUC\n",
    "\t\tauc_score   = roc_auc_score(self.y_test, yhat_probs)\n",
    "\t\t# confusion matrix\n",
    "\t\tmatrix      = confusion_matrix(self.y_test, yhat_classes)\n",
    "\t\tcr          = classification_report(self.y_test,yhat_classes,digits=3)\n",
    "\n",
    "\t\treturn_dict = {\"accuracy\":accuracy,\"precision\":precision,\"recall\":recall,\"f1\":f1,\"loss\":loss,\"kappa\":kappa,\"auc\":auc_score,\"confusion_matrix\":matrix,\"classification_report\":cr}\n",
    "\n",
    "\t\tif verbose>0:\n",
    "\t\t\tprint('Accuracy: %f' % accuracy)\n",
    "\t\t\tprint('Precision: %f' % precision)\n",
    "\t\t\tprint('Recall: %f' % recall)\n",
    "\t\t\tprint('F1 score: %f' % f1)\n",
    "\t\t\tprint('Loss: %f' % loss)\n",
    "\t\t\tprint('Cohens kappa: %f' % kappa)\n",
    "\t\t\tprint('ROC AUC: %f' % auc_score)\n",
    "\t\t\tprint(matrix)\n",
    "\t\t\tprint(cr)\n",
    "\n",
    "\t\treturn return_dict\n",
    "\t\"\"\"show results visually\"\"\"\n",
    "\tdef plot(self):\n",
    "\t\t# predict probabilities for test set\n",
    "\t\tyhat_probs = self.model.predict(self.X_test, verbose=0)\n",
    "\n",
    "\t\t# reduce to 1d array\n",
    "\t\tyhat_probs = yhat_probs[:, 0]\n",
    "\n",
    "\t\tyhat_classes = yhat_probs.copy()\n",
    "\t\tyhat_classes[yhat_classes>=self.threshold]=np.float64(1)\n",
    "\t\tyhat_classes[yhat_classes<self.threshold]=np.float64(0)\n",
    "\n",
    "\t\tFPR, TPR, _ = roc_curve(self.y_test, yhat_probs)\n",
    "\t\tAUC = auc(FPR, TPR)\n",
    "\n",
    "\t\tplt.figure(\"Neural Network ROC\")\n",
    "\t\tplt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % AUC)\n",
    "\t\tplt.plot([0, 1], [0, 1], 'r--')\n",
    "\t\tplt.xlim([0.0, 1.0])\n",
    "\t\tplt.ylim([0.0, 1.02])\n",
    "\t\tplt.xlabel('False Positive Rate')\n",
    "\t\tplt.ylabel('True Positive Rate')\n",
    "\t\tplt.title('ROC Curve')\n",
    "\t\tplt.legend(loc=\"lower right\")\n",
    "\n",
    "\t\t# summarize history for accuracy\n",
    "\t\tplt.figure(\"Neural Network Accuracy Graph\")\n",
    "\t\tplt.plot(self.model_history.history['acc'])\n",
    "\t\tplt.plot(self.model_history.history['val_acc'])\n",
    "\t\tplt.title('Model Accuracy')\n",
    "\t\tplt.ylabel('Accuracy')\n",
    "\t\tplt.xlabel('Epoch')\n",
    "\t\tplt.legend(['Train', 'Test'], loc='upper left')\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\"\"\"save the model in a file. return true if successful, false- otherwise\"\"\"\n",
    "\tdef save_model(self, models_name='model'):\n",
    "\t\tmodel_file  = models_name+\".h5\"\n",
    "\t\tscaler_file = models_name+\"_scaler.sav\"\n",
    "\t\tif self.model_history is not None:\n",
    "\t\t\t# save model and architecture to single file\n",
    "\t\t\tself.model.save(model_file)\n",
    "\t\t\tpickle.dump(self.scaler, open(scaler_file, 'wb'))\n",
    "\t\t\tprint(\"Saved model to disk\")\n",
    "\t\t\treturn True\n",
    "\t\treturn False\n",
    "\t\"\"\"load model from an existing path- return true if model file existing and loaded, false- otherwise\"\"\"\n",
    "\tdef load_model(self, models_name='model'):\n",
    "\t\tmodel_file  = models_name+\".h5\"\n",
    "\t\tscaler_file = models_name+\"_scaler.sav\"\n",
    "\t\tif os.path.isfile(model_file) and os.path.isfile(scaler_file):\n",
    "\t\t\t# load model and architecture from single file\n",
    "\t\t\tself.model = tf.keras.models.load_model(model_file)\n",
    "\t\t\tself.scaler = pickle.load(open(scaler_file, 'rb'))\n",
    "\t\t\tprint(\"Loaded model from disk\")\n",
    "\t\t\t# self.model.summary()\n",
    "\t\t\treturn True\n",
    "\t\treturn False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Checking features - new_BR_robust_entropy_numip_distip_noccr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliel.gez\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch 1/20000\n",
      "135/135 [==============================] - 2s 7ms/step - loss: 0.3452 - acc: 0.8495 - val_loss: 0.1835 - val_acc: 0.9372\n",
      "Epoch 2/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.1571 - acc: 0.9448 - val_loss: 0.1489 - val_acc: 0.9421\n",
      "Epoch 3/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.1312 - acc: 0.9540 - val_loss: 0.1235 - val_acc: 0.9546\n",
      "Epoch 4/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.1189 - acc: 0.9554 - val_loss: 0.1109 - val_acc: 0.9568\n",
      "Epoch 5/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.1064 - acc: 0.9603 - val_loss: 0.1023 - val_acc: 0.9604\n",
      "Epoch 6/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.1005 - acc: 0.9631 - val_loss: 0.1032 - val_acc: 0.9613\n",
      "Epoch 7/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.1002 - acc: 0.9633 - val_loss: 0.1014 - val_acc: 0.9635\n",
      "Epoch 8/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0933 - acc: 0.9661 - val_loss: 0.0927 - val_acc: 0.9640\n",
      "Epoch 9/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0893 - acc: 0.9672 - val_loss: 0.0956 - val_acc: 0.9644\n",
      "Epoch 10/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0879 - acc: 0.9682 - val_loss: 0.0961 - val_acc: 0.9666\n",
      "Epoch 11/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0855 - acc: 0.9689 - val_loss: 0.0938 - val_acc: 0.9671\n",
      "Epoch 12/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0839 - acc: 0.9703 - val_loss: 0.0897 - val_acc: 0.9706\n",
      "Epoch 13/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0807 - acc: 0.9718 - val_loss: 0.0932 - val_acc: 0.9644\n",
      "Epoch 14/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0810 - acc: 0.9698 - val_loss: 0.0891 - val_acc: 0.9662\n",
      "Epoch 15/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0819 - acc: 0.9708 - val_loss: 0.0897 - val_acc: 0.9688\n",
      "Epoch 16/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0744 - acc: 0.9738 - val_loss: 0.0791 - val_acc: 0.9733\n",
      "Epoch 17/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0740 - acc: 0.9742 - val_loss: 0.0850 - val_acc: 0.9720\n",
      "Epoch 18/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0725 - acc: 0.9736 - val_loss: 0.0889 - val_acc: 0.9671\n",
      "Epoch 19/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0723 - acc: 0.9745 - val_loss: 0.0777 - val_acc: 0.9733\n",
      "Epoch 20/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0718 - acc: 0.9744 - val_loss: 0.0784 - val_acc: 0.9742\n",
      "Epoch 21/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0718 - acc: 0.9744 - val_loss: 0.0791 - val_acc: 0.9715\n",
      "Epoch 22/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0678 - acc: 0.9770 - val_loss: 0.0784 - val_acc: 0.9742\n",
      "Epoch 23/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0657 - acc: 0.9774 - val_loss: 0.0721 - val_acc: 0.9751\n",
      "Epoch 24/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0673 - acc: 0.9776 - val_loss: 0.0760 - val_acc: 0.9720\n",
      "Epoch 25/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0670 - acc: 0.9770 - val_loss: 0.0845 - val_acc: 0.9737\n",
      "Epoch 26/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0650 - acc: 0.9774 - val_loss: 0.0708 - val_acc: 0.9751\n",
      "Epoch 27/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0670 - acc: 0.9763 - val_loss: 0.0755 - val_acc: 0.9720\n",
      "Epoch 28/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0625 - acc: 0.9780 - val_loss: 0.0881 - val_acc: 0.9711\n",
      "Epoch 29/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0635 - acc: 0.9769 - val_loss: 0.0823 - val_acc: 0.9742\n",
      "Epoch 30/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0614 - acc: 0.9782 - val_loss: 0.0813 - val_acc: 0.9724\n",
      "Epoch 31/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0619 - acc: 0.9774 - val_loss: 0.0754 - val_acc: 0.9777\n",
      "Epoch 32/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0594 - acc: 0.9798 - val_loss: 0.0690 - val_acc: 0.9764\n",
      "Epoch 33/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0575 - acc: 0.9800 - val_loss: 0.0735 - val_acc: 0.9733\n",
      "Epoch 34/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0583 - acc: 0.9796 - val_loss: 0.0752 - val_acc: 0.9769\n",
      "Epoch 35/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0576 - acc: 0.9797 - val_loss: 0.0792 - val_acc: 0.9702\n",
      "Epoch 36/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0588 - acc: 0.9795 - val_loss: 0.0979 - val_acc: 0.9653\n",
      "Epoch 37/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0581 - acc: 0.9793 - val_loss: 0.0727 - val_acc: 0.9751\n",
      "Epoch 38/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0565 - acc: 0.9800 - val_loss: 0.0731 - val_acc: 0.9764\n",
      "Epoch 39/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0532 - acc: 0.9816 - val_loss: 0.0705 - val_acc: 0.9773\n",
      "Epoch 40/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0516 - acc: 0.9820 - val_loss: 0.0724 - val_acc: 0.9786\n",
      "Epoch 41/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0530 - acc: 0.9822 - val_loss: 0.0831 - val_acc: 0.9764\n",
      "Epoch 42/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0528 - acc: 0.9818 - val_loss: 0.0662 - val_acc: 0.9777\n",
      "Epoch 43/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0524 - acc: 0.9816 - val_loss: 0.0775 - val_acc: 0.9760\n",
      "Epoch 44/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0539 - acc: 0.9817 - val_loss: 0.0763 - val_acc: 0.9724\n",
      "Epoch 45/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0504 - acc: 0.9835 - val_loss: 0.0733 - val_acc: 0.9737\n",
      "Epoch 46/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0493 - acc: 0.9827 - val_loss: 0.0622 - val_acc: 0.9773\n",
      "Epoch 47/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0504 - acc: 0.9824 - val_loss: 0.0752 - val_acc: 0.9737\n",
      "Epoch 48/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0502 - acc: 0.9822 - val_loss: 0.0700 - val_acc: 0.9795\n",
      "Epoch 49/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0499 - acc: 0.9826 - val_loss: 0.0826 - val_acc: 0.9769\n",
      "Epoch 50/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0478 - acc: 0.9836 - val_loss: 0.0642 - val_acc: 0.9769\n",
      "Epoch 51/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0461 - acc: 0.9841 - val_loss: 0.0677 - val_acc: 0.9791\n",
      "Epoch 52/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0477 - acc: 0.9839 - val_loss: 0.0778 - val_acc: 0.9746\n",
      "Epoch 53/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0495 - acc: 0.9827 - val_loss: 0.0760 - val_acc: 0.9800\n",
      "Epoch 54/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0482 - acc: 0.9838 - val_loss: 0.0802 - val_acc: 0.9760\n",
      "Epoch 55/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0453 - acc: 0.9845 - val_loss: 0.0640 - val_acc: 0.9809\n",
      "Epoch 56/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0441 - acc: 0.9853 - val_loss: 0.0648 - val_acc: 0.9791\n",
      "Epoch 57/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0442 - acc: 0.9854 - val_loss: 0.0685 - val_acc: 0.9800\n",
      "Epoch 58/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0451 - acc: 0.9840 - val_loss: 0.0630 - val_acc: 0.9795\n",
      "Epoch 59/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0429 - acc: 0.9846 - val_loss: 0.0727 - val_acc: 0.9809\n",
      "Epoch 60/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0487 - acc: 0.9832 - val_loss: 0.0640 - val_acc: 0.9800\n",
      "Epoch 61/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0437 - acc: 0.9845 - val_loss: 0.0746 - val_acc: 0.9786\n",
      "Epoch 62/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0439 - acc: 0.9845 - val_loss: 0.0653 - val_acc: 0.9795\n",
      "Epoch 63/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0428 - acc: 0.9851 - val_loss: 0.0710 - val_acc: 0.9795\n",
      "Epoch 64/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0410 - acc: 0.9856 - val_loss: 0.0599 - val_acc: 0.9835\n",
      "Epoch 65/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0433 - acc: 0.9848 - val_loss: 0.0618 - val_acc: 0.9813\n",
      "Epoch 66/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0422 - acc: 0.9852 - val_loss: 0.0582 - val_acc: 0.9818\n",
      "Epoch 67/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0400 - acc: 0.9862 - val_loss: 0.0714 - val_acc: 0.9795\n",
      "Epoch 68/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0392 - acc: 0.9856 - val_loss: 0.0608 - val_acc: 0.9809\n",
      "Epoch 69/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0397 - acc: 0.9862 - val_loss: 0.0607 - val_acc: 0.9818\n",
      "Epoch 70/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0415 - acc: 0.9854 - val_loss: 0.0538 - val_acc: 0.9840\n",
      "Epoch 71/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0374 - acc: 0.9870 - val_loss: 0.0612 - val_acc: 0.9809\n",
      "Epoch 72/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0410 - acc: 0.9862 - val_loss: 0.0653 - val_acc: 0.9809\n",
      "Epoch 73/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0368 - acc: 0.9876 - val_loss: 0.0848 - val_acc: 0.9786\n",
      "Epoch 74/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0434 - acc: 0.9851 - val_loss: 0.0678 - val_acc: 0.9791\n",
      "Epoch 75/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0354 - acc: 0.9881 - val_loss: 0.0616 - val_acc: 0.9813\n",
      "Epoch 76/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0348 - acc: 0.9878 - val_loss: 0.0674 - val_acc: 0.9804\n",
      "Epoch 77/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0383 - acc: 0.9870 - val_loss: 0.0651 - val_acc: 0.9782\n",
      "Epoch 78/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0388 - acc: 0.9861 - val_loss: 0.0563 - val_acc: 0.9826\n",
      "Epoch 79/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0348 - acc: 0.9882 - val_loss: 0.0609 - val_acc: 0.9826\n",
      "Epoch 80/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0346 - acc: 0.9881 - val_loss: 0.0551 - val_acc: 0.9840\n",
      "Epoch 81/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0372 - acc: 0.9864 - val_loss: 0.0730 - val_acc: 0.9800\n",
      "Epoch 82/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0345 - acc: 0.9878 - val_loss: 0.0574 - val_acc: 0.9831\n",
      "Epoch 83/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0361 - acc: 0.9872 - val_loss: 0.0632 - val_acc: 0.9818\n",
      "Epoch 84/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0376 - acc: 0.9863 - val_loss: 0.0762 - val_acc: 0.9791\n",
      "Epoch 85/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0357 - acc: 0.9873 - val_loss: 0.0699 - val_acc: 0.9782\n",
      "Epoch 86/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0320 - acc: 0.9884 - val_loss: 0.0721 - val_acc: 0.9809\n",
      "Epoch 87/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0329 - acc: 0.9890 - val_loss: 0.0645 - val_acc: 0.9782\n",
      "Epoch 88/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0330 - acc: 0.9887 - val_loss: 0.0571 - val_acc: 0.9835\n",
      "Epoch 89/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0334 - acc: 0.9879 - val_loss: 0.0703 - val_acc: 0.9791\n",
      "Epoch 90/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0320 - acc: 0.9888 - val_loss: 0.0671 - val_acc: 0.9826\n",
      "Epoch 91/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0323 - acc: 0.9886 - val_loss: 0.0636 - val_acc: 0.9840\n",
      "Epoch 92/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0302 - acc: 0.9898 - val_loss: 0.0686 - val_acc: 0.9791\n",
      "Epoch 93/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0303 - acc: 0.9893 - val_loss: 0.0552 - val_acc: 0.9844\n",
      "Epoch 94/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0335 - acc: 0.9882 - val_loss: 0.0687 - val_acc: 0.9804\n",
      "Epoch 95/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0321 - acc: 0.9888 - val_loss: 0.0624 - val_acc: 0.9831\n",
      "Epoch 96/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0304 - acc: 0.9898 - val_loss: 0.0688 - val_acc: 0.9835\n",
      "Epoch 97/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0338 - acc: 0.9885 - val_loss: 0.0618 - val_acc: 0.9813\n",
      "Epoch 98/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0326 - acc: 0.9889 - val_loss: 0.0701 - val_acc: 0.9795\n",
      "Epoch 99/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0338 - acc: 0.9878 - val_loss: 0.0774 - val_acc: 0.9809\n",
      "Epoch 100/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0313 - acc: 0.9885 - val_loss: 0.0685 - val_acc: 0.9804\n",
      "Epoch 101/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0290 - acc: 0.9900 - val_loss: 0.0551 - val_acc: 0.9858\n",
      "Epoch 102/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0295 - acc: 0.9899 - val_loss: 0.0606 - val_acc: 0.9818\n",
      "Epoch 103/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0275 - acc: 0.9904 - val_loss: 0.0954 - val_acc: 0.9822\n",
      "Epoch 104/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0281 - acc: 0.9902 - val_loss: 0.0625 - val_acc: 0.9835\n",
      "Epoch 105/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0258 - acc: 0.9906 - val_loss: 0.0715 - val_acc: 0.9840\n",
      "Epoch 106/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0267 - acc: 0.9911 - val_loss: 0.0680 - val_acc: 0.9822\n",
      "Epoch 107/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0293 - acc: 0.9895 - val_loss: 0.0623 - val_acc: 0.9840\n",
      "Epoch 108/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0264 - acc: 0.9903 - val_loss: 0.0597 - val_acc: 0.9831\n",
      "Epoch 109/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0268 - acc: 0.9906 - val_loss: 0.0655 - val_acc: 0.9835\n",
      "Epoch 110/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0263 - acc: 0.9910 - val_loss: 0.0788 - val_acc: 0.9809\n",
      "Epoch 111/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0252 - acc: 0.9904 - val_loss: 0.0782 - val_acc: 0.9826\n",
      "Epoch 112/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0234 - acc: 0.9914 - val_loss: 0.0661 - val_acc: 0.9862\n",
      "Epoch 113/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0240 - acc: 0.9924 - val_loss: 0.0825 - val_acc: 0.9818\n",
      "Epoch 114/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0237 - acc: 0.9919 - val_loss: 0.0907 - val_acc: 0.9786\n",
      "Epoch 115/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0284 - acc: 0.9910 - val_loss: 0.0725 - val_acc: 0.9826\n",
      "Epoch 116/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0275 - acc: 0.9902 - val_loss: 0.0664 - val_acc: 0.9840\n",
      "Epoch 117/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0240 - acc: 0.9911 - val_loss: 0.0659 - val_acc: 0.9835\n",
      "Epoch 118/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0242 - acc: 0.9914 - val_loss: 0.0770 - val_acc: 0.9813\n",
      "Epoch 119/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0247 - acc: 0.9915 - val_loss: 0.0712 - val_acc: 0.9818\n",
      "Epoch 120/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0241 - acc: 0.9911 - val_loss: 0.0647 - val_acc: 0.9835\n",
      "Epoch 121/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0265 - acc: 0.9907 - val_loss: 0.0762 - val_acc: 0.9826\n",
      "Epoch 122/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0251 - acc: 0.9910 - val_loss: 0.0692 - val_acc: 0.9853\n",
      "Epoch 123/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0236 - acc: 0.9910 - val_loss: 0.0986 - val_acc: 0.9818\n",
      "Epoch 124/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0239 - acc: 0.9920 - val_loss: 0.0645 - val_acc: 0.9858\n",
      "Epoch 125/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0191 - acc: 0.9933 - val_loss: 0.0634 - val_acc: 0.9858\n",
      "Epoch 126/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0235 - acc: 0.9917 - val_loss: 0.0696 - val_acc: 0.9853\n",
      "Epoch 127/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0220 - acc: 0.9925 - val_loss: 0.0809 - val_acc: 0.9835\n",
      "Epoch 128/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0228 - acc: 0.9921 - val_loss: 0.1346 - val_acc: 0.9760\n",
      "Epoch 129/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0229 - acc: 0.9918 - val_loss: 0.0806 - val_acc: 0.9849\n",
      "Epoch 130/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0242 - acc: 0.9921 - val_loss: 0.0783 - val_acc: 0.9826\n",
      "Epoch 131/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0187 - acc: 0.9930 - val_loss: 0.1006 - val_acc: 0.9804\n",
      "Epoch 132/20000\n",
      "135/135 [==============================] - 1s 7ms/step - loss: 0.0187 - acc: 0.9935 - val_loss: 0.0803 - val_acc: 0.9809\n",
      "Epoch 133/20000\n",
      "135/135 [==============================] - ETA: 0s - loss: 0.0200 - acc: 0.993 - 1s 7ms/step - loss: 0.0204 - acc: 0.9930 - val_loss: 0.0654 - val_acc: 0.9840\n",
      "Epoch 134/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0270 - acc: 0.9900 - val_loss: 0.0924 - val_acc: 0.9809\n",
      "Epoch 135/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0208 - acc: 0.9928 - val_loss: 0.0751 - val_acc: 0.9840\n",
      "Epoch 136/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0210 - acc: 0.9926 - val_loss: 0.0725 - val_acc: 0.9858\n",
      "Epoch 137/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0201 - acc: 0.9927 - val_loss: 0.0914 - val_acc: 0.9840\n",
      "Epoch 138/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0181 - acc: 0.9939 - val_loss: 0.0851 - val_acc: 0.9786\n",
      "Epoch 139/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0180 - acc: 0.9937 - val_loss: 0.1006 - val_acc: 0.9840\n",
      "Epoch 140/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0192 - acc: 0.9929 - val_loss: 0.0736 - val_acc: 0.9853\n",
      "Epoch 141/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0212 - acc: 0.9920 - val_loss: 0.0899 - val_acc: 0.9813\n",
      "Epoch 142/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0206 - acc: 0.9923 - val_loss: 0.0847 - val_acc: 0.9835\n",
      "Epoch 143/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0236 - acc: 0.9919 - val_loss: 0.0810 - val_acc: 0.9826\n",
      "Epoch 144/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0227 - acc: 0.9911 - val_loss: 0.0864 - val_acc: 0.9818\n",
      "Epoch 145/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0194 - acc: 0.9924 - val_loss: 0.0795 - val_acc: 0.9844\n",
      "Epoch 146/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0171 - acc: 0.9936 - val_loss: 0.0902 - val_acc: 0.9835\n",
      "Epoch 147/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0152 - acc: 0.9943 - val_loss: 0.1040 - val_acc: 0.9831\n",
      "Epoch 148/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0169 - acc: 0.9935 - val_loss: 0.0979 - val_acc: 0.9831\n",
      "Epoch 149/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0223 - acc: 0.9913 - val_loss: 0.0947 - val_acc: 0.9804\n",
      "Epoch 150/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0207 - acc: 0.9929 - val_loss: 0.0659 - val_acc: 0.9884\n",
      "Epoch 151/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0156 - acc: 0.9949 - val_loss: 0.0726 - val_acc: 0.9849\n",
      "Epoch 152/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0189 - acc: 0.9932 - val_loss: 0.0706 - val_acc: 0.9835\n",
      "Epoch 153/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0176 - acc: 0.9940 - val_loss: 0.0906 - val_acc: 0.9813\n",
      "Epoch 154/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0159 - acc: 0.9938 - val_loss: 0.0881 - val_acc: 0.9840\n",
      "Epoch 155/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0188 - acc: 0.9924 - val_loss: 0.0760 - val_acc: 0.9831\n",
      "Epoch 156/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0143 - acc: 0.9949 - val_loss: 0.0789 - val_acc: 0.9862\n",
      "Epoch 157/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0160 - acc: 0.9943 - val_loss: 0.1019 - val_acc: 0.9818\n",
      "Epoch 158/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0172 - acc: 0.9936 - val_loss: 0.1100 - val_acc: 0.9813\n",
      "Epoch 159/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0172 - acc: 0.9936 - val_loss: 0.1155 - val_acc: 0.9835\n",
      "Epoch 160/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0203 - acc: 0.9925 - val_loss: 0.1306 - val_acc: 0.9795\n",
      "Epoch 161/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0206 - acc: 0.9929 - val_loss: 0.1161 - val_acc: 0.9858\n",
      "Epoch 162/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0141 - acc: 0.9946 - val_loss: 0.0885 - val_acc: 0.9844\n",
      "Epoch 163/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0146 - acc: 0.9948 - val_loss: 0.0981 - val_acc: 0.9835\n",
      "Epoch 164/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0134 - acc: 0.9950 - val_loss: 0.0947 - val_acc: 0.9853\n",
      "Epoch 165/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0139 - acc: 0.9948 - val_loss: 0.1105 - val_acc: 0.9853\n",
      "Epoch 166/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0147 - acc: 0.9942 - val_loss: 0.0978 - val_acc: 0.9835\n",
      "Epoch 167/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0131 - acc: 0.9954 - val_loss: 0.1129 - val_acc: 0.9853\n",
      "Epoch 168/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0213 - acc: 0.9925 - val_loss: 0.1241 - val_acc: 0.9813\n",
      "Epoch 169/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0199 - acc: 0.9925 - val_loss: 0.0993 - val_acc: 0.9831\n",
      "Epoch 170/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0127 - acc: 0.9954 - val_loss: 0.1024 - val_acc: 0.9866\n",
      "Epoch 1/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0319 - acc: 0.9920 - val_loss: 0.0204 - val_acc: 0.9924\n",
      "Epoch 2/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0234 - acc: 0.9917 - val_loss: 0.0275 - val_acc: 0.9907\n",
      "Epoch 3/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0197 - acc: 0.9936 - val_loss: 0.0196 - val_acc: 0.9933\n",
      "Epoch 4/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0166 - acc: 0.9945 - val_loss: 0.0190 - val_acc: 0.9964\n",
      "Epoch 5/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0154 - acc: 0.9944 - val_loss: 0.0290 - val_acc: 0.9889\n",
      "Epoch 6/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0148 - acc: 0.9950 - val_loss: 0.0251 - val_acc: 0.9938\n",
      "Epoch 7/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0149 - acc: 0.9948 - val_loss: 0.0281 - val_acc: 0.9907\n",
      "Epoch 8/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0151 - acc: 0.9946 - val_loss: 0.0240 - val_acc: 0.9942\n",
      "Epoch 9/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0141 - acc: 0.9950 - val_loss: 0.0243 - val_acc: 0.9942\n",
      "Epoch 10/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0156 - acc: 0.9943 - val_loss: 0.0302 - val_acc: 0.9938\n",
      "Epoch 11/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0169 - acc: 0.9934 - val_loss: 0.0255 - val_acc: 0.9938\n",
      "Epoch 12/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0164 - acc: 0.9939 - val_loss: 0.0276 - val_acc: 0.9960\n",
      "Epoch 13/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0146 - acc: 0.9947 - val_loss: 0.0332 - val_acc: 0.9929\n",
      "Epoch 14/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0144 - acc: 0.9949 - val_loss: 0.0181 - val_acc: 0.9955\n",
      "Epoch 15/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0175 - acc: 0.9937 - val_loss: 0.0278 - val_acc: 0.9911\n",
      "Epoch 16/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0202 - acc: 0.9924 - val_loss: 0.0372 - val_acc: 0.9924\n",
      "Epoch 17/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0228 - acc: 0.9915 - val_loss: 0.0397 - val_acc: 0.9898\n",
      "Epoch 18/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0182 - acc: 0.9937 - val_loss: 0.0379 - val_acc: 0.9915\n",
      "Epoch 19/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0133 - acc: 0.9951 - val_loss: 0.0204 - val_acc: 0.9942\n",
      "Epoch 20/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0122 - acc: 0.9952 - val_loss: 0.0325 - val_acc: 0.9920\n",
      "Epoch 21/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0113 - acc: 0.9961 - val_loss: 0.0290 - val_acc: 0.9942\n",
      "Epoch 22/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0122 - acc: 0.9957 - val_loss: 0.0305 - val_acc: 0.9942\n",
      "Epoch 23/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0327 - val_acc: 0.9938\n",
      "Epoch 24/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0141 - acc: 0.9948 - val_loss: 0.0343 - val_acc: 0.9929\n",
      "Epoch 25/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0109 - acc: 0.9960 - val_loss: 0.0443 - val_acc: 0.9907\n",
      "Epoch 26/20000\n",
      "135/135 [==============================] - 1s 7ms/step - loss: 0.0110 - acc: 0.9960 - val_loss: 0.0377 - val_acc: 0.9929\n",
      "Epoch 27/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0128 - acc: 0.9954 - val_loss: 0.0275 - val_acc: 0.9929\n",
      "Epoch 28/20000\n",
      "135/135 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.9942 - val_loss: 0.0306 - val_acc: 0.9929\n",
      "Epoch 29/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0220 - acc: 0.9924 - val_loss: 0.0273 - val_acc: 0.9915\n",
      "Epoch 30/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0161 - acc: 0.9949 - val_loss: 0.0215 - val_acc: 0.9911\n",
      "Epoch 31/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0124 - acc: 0.9957 - val_loss: 0.0332 - val_acc: 0.9902\n",
      "Epoch 32/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0134 - acc: 0.9953 - val_loss: 0.0223 - val_acc: 0.9942\n",
      "Epoch 33/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0112 - acc: 0.9959 - val_loss: 0.0296 - val_acc: 0.9911\n",
      "Epoch 34/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0108 - acc: 0.9959 - val_loss: 0.0185 - val_acc: 0.9960\n",
      "Epoch 35/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0126 - acc: 0.9961 - val_loss: 0.0219 - val_acc: 0.9933\n",
      "Epoch 36/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0125 - acc: 0.9951 - val_loss: 0.0244 - val_acc: 0.9911\n",
      "Epoch 37/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0143 - acc: 0.9944 - val_loss: 0.0249 - val_acc: 0.9911\n",
      "Epoch 38/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0103 - acc: 0.9964 - val_loss: 0.0443 - val_acc: 0.9875\n",
      "Epoch 39/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0123 - acc: 0.9953 - val_loss: 0.0206 - val_acc: 0.9942\n",
      "Epoch 40/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0098 - acc: 0.9965 - val_loss: 0.0422 - val_acc: 0.9866\n",
      "Epoch 41/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0104 - acc: 0.9964 - val_loss: 0.0248 - val_acc: 0.9929\n",
      "Epoch 42/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0131 - acc: 0.9953 - val_loss: 0.0411 - val_acc: 0.9911\n",
      "Epoch 43/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0261 - val_acc: 0.9933\n",
      "Epoch 44/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0125 - acc: 0.9955 - val_loss: 0.0451 - val_acc: 0.9902\n",
      "Epoch 45/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0157 - acc: 0.9947 - val_loss: 0.0268 - val_acc: 0.9924\n",
      "Epoch 46/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0157 - acc: 0.9944 - val_loss: 0.0245 - val_acc: 0.9924\n",
      "Epoch 47/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0107 - acc: 0.9962 - val_loss: 0.0282 - val_acc: 0.9929\n",
      "Epoch 48/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0093 - acc: 0.9968 - val_loss: 0.0281 - val_acc: 0.9924\n",
      "Epoch 49/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0085 - acc: 0.9969 - val_loss: 0.0290 - val_acc: 0.9933\n",
      "Epoch 50/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0115 - acc: 0.9960 - val_loss: 0.0349 - val_acc: 0.9915\n",
      "Epoch 51/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0114 - acc: 0.9960 - val_loss: 0.0296 - val_acc: 0.9924\n",
      "Epoch 52/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0111 - acc: 0.9959 - val_loss: 0.0304 - val_acc: 0.9915\n",
      "Epoch 53/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0116 - acc: 0.9961 - val_loss: 0.0436 - val_acc: 0.9898\n",
      "Epoch 54/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0111 - acc: 0.9960 - val_loss: 0.0349 - val_acc: 0.9929\n",
      "Epoch 55/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0474 - val_acc: 0.9907\n",
      "Epoch 56/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0113 - acc: 0.9955 - val_loss: 0.0512 - val_acc: 0.9911\n",
      "Epoch 57/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0174 - acc: 0.9945 - val_loss: 0.0600 - val_acc: 0.9871\n",
      "Epoch 58/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0137 - acc: 0.9953 - val_loss: 0.0429 - val_acc: 0.9907\n",
      "Epoch 59/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0099 - acc: 0.9966 - val_loss: 0.0240 - val_acc: 0.9920\n",
      "Epoch 60/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0080 - acc: 0.9970 - val_loss: 0.0358 - val_acc: 0.9889\n",
      "Epoch 61/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0086 - acc: 0.9971 - val_loss: 0.0214 - val_acc: 0.9933\n",
      "Epoch 62/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0088 - acc: 0.9971 - val_loss: 0.0414 - val_acc: 0.9907\n",
      "Epoch 63/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0302 - val_acc: 0.9893\n",
      "Epoch 64/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0160 - acc: 0.9955 - val_loss: 0.1089 - val_acc: 0.9737\n",
      "Epoch 65/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0258 - acc: 0.9908 - val_loss: 0.0340 - val_acc: 0.9898\n",
      "Epoch 66/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0134 - acc: 0.9952 - val_loss: 0.0493 - val_acc: 0.9866\n",
      "Epoch 67/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0092 - acc: 0.9969 - val_loss: 0.0344 - val_acc: 0.9898\n",
      "Epoch 68/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0074 - acc: 0.9974 - val_loss: 0.0310 - val_acc: 0.9915\n",
      "Epoch 69/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0348 - val_acc: 0.9911\n",
      "Epoch 70/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0092 - acc: 0.9968 - val_loss: 0.0319 - val_acc: 0.9911\n",
      "Epoch 71/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0085 - acc: 0.9969 - val_loss: 0.0277 - val_acc: 0.9920\n",
      "Epoch 72/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0422 - val_acc: 0.9893\n",
      "Epoch 73/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0101 - acc: 0.9961 - val_loss: 0.0308 - val_acc: 0.9938\n",
      "Epoch 74/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0388 - val_acc: 0.9898\n",
      "Epoch 75/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0109 - acc: 0.9963 - val_loss: 0.0379 - val_acc: 0.9911\n",
      "Epoch 76/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0157 - acc: 0.9942 - val_loss: 0.0389 - val_acc: 0.9907\n",
      "Epoch 77/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0109 - acc: 0.9956 - val_loss: 0.0524 - val_acc: 0.9902\n",
      "Epoch 78/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 0.0489 - val_acc: 0.9915\n",
      "Epoch 79/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0087 - acc: 0.9966 - val_loss: 0.0312 - val_acc: 0.9920\n",
      "Epoch 80/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0086 - acc: 0.9969 - val_loss: 0.0413 - val_acc: 0.9911\n",
      "Epoch 81/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0387 - val_acc: 0.9907\n",
      "Epoch 82/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0074 - acc: 0.9974 - val_loss: 0.0294 - val_acc: 0.9924\n",
      "Epoch 83/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0081 - acc: 0.9970 - val_loss: 0.0344 - val_acc: 0.9920\n",
      "Epoch 84/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0097 - acc: 0.9962 - val_loss: 0.0311 - val_acc: 0.9920\n",
      "Epoch 85/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0095 - acc: 0.9964 - val_loss: 0.0350 - val_acc: 0.9924\n",
      "Epoch 86/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0102 - acc: 0.9962 - val_loss: 0.0415 - val_acc: 0.9915\n",
      "Epoch 87/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0190 - acc: 0.9944 - val_loss: 0.0933 - val_acc: 0.9866\n",
      "Epoch 88/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0146 - acc: 0.9947 - val_loss: 0.0396 - val_acc: 0.9915\n",
      "Epoch 89/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0095 - acc: 0.9965 - val_loss: 0.0310 - val_acc: 0.9924\n",
      "Epoch 90/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0064 - acc: 0.9976 - val_loss: 0.0361 - val_acc: 0.9907\n",
      "Epoch 91/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0144 - acc: 0.9951 - val_loss: 0.0593 - val_acc: 0.9858\n",
      "Epoch 92/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0124 - acc: 0.9955 - val_loss: 0.0307 - val_acc: 0.9933\n",
      "Epoch 93/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0071 - acc: 0.9974 - val_loss: 0.0369 - val_acc: 0.9915\n",
      "Epoch 94/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0356 - val_acc: 0.9920\n",
      "Epoch 95/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0496 - val_acc: 0.9871\n",
      "Epoch 96/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0092 - acc: 0.9958 - val_loss: 0.0480 - val_acc: 0.9893\n",
      "Epoch 97/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0111 - acc: 0.9958 - val_loss: 0.0403 - val_acc: 0.9902\n",
      "Epoch 98/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0435 - val_acc: 0.9902\n",
      "Epoch 99/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0126 - acc: 0.9955 - val_loss: 0.0290 - val_acc: 0.9911\n",
      "Epoch 100/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0131 - acc: 0.9951 - val_loss: 0.0289 - val_acc: 0.9902\n",
      "Epoch 101/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0448 - val_acc: 0.9898\n",
      "Epoch 102/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0402 - val_acc: 0.9902\n",
      "Epoch 103/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0282 - val_acc: 0.9938\n",
      "Epoch 104/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0059 - acc: 0.9977 - val_loss: 0.0354 - val_acc: 0.9911\n",
      "Epoch 105/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0068 - acc: 0.9974 - val_loss: 0.0474 - val_acc: 0.9875\n",
      "Epoch 106/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0078 - acc: 0.9973 - val_loss: 0.0397 - val_acc: 0.9889\n",
      "Epoch 107/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0058 - acc: 0.9978 - val_loss: 0.0384 - val_acc: 0.9911\n",
      "Epoch 108/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0077 - acc: 0.9972 - val_loss: 0.0416 - val_acc: 0.9911\n",
      "Epoch 109/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0103 - acc: 0.9961 - val_loss: 0.0382 - val_acc: 0.9911\n",
      "Epoch 110/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0483 - val_acc: 0.9889\n",
      "Epoch 111/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0406 - val_acc: 0.9911\n",
      "Epoch 112/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0104 - acc: 0.9963 - val_loss: 0.0546 - val_acc: 0.9889\n",
      "Epoch 113/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0142 - acc: 0.9954 - val_loss: 0.0577 - val_acc: 0.9875\n",
      "Epoch 114/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0134 - acc: 0.9952 - val_loss: 0.0371 - val_acc: 0.9893\n",
      "Epoch 1/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0075 - val_acc: 0.9969\n",
      "Epoch 2/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0104 - acc: 0.9962 - val_loss: 0.0174 - val_acc: 0.9929\n",
      "Epoch 3/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0117 - acc: 0.9961 - val_loss: 0.0144 - val_acc: 0.9933\n",
      "Epoch 4/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0222 - acc: 0.9932 - val_loss: 0.0223 - val_acc: 0.9920\n",
      "Epoch 5/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0097 - acc: 0.9964 - val_loss: 0.0069 - val_acc: 0.9978\n",
      "Epoch 6/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0095 - val_acc: 0.9969\n",
      "Epoch 7/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0078 - acc: 0.9973 - val_loss: 0.0082 - val_acc: 0.9955\n",
      "Epoch 8/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0079 - acc: 0.9970 - val_loss: 0.0113 - val_acc: 0.9951\n",
      "Epoch 9/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0066 - acc: 0.9972 - val_loss: 0.0087 - val_acc: 0.9969\n",
      "Epoch 10/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0055 - acc: 0.9979 - val_loss: 0.0083 - val_acc: 0.9960\n",
      "Epoch 11/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0134 - acc: 0.9954 - val_loss: 0.0108 - val_acc: 0.9964\n",
      "Epoch 12/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0066 - acc: 0.9975 - val_loss: 0.0094 - val_acc: 0.9960\n",
      "Epoch 13/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0060 - acc: 0.9977 - val_loss: 0.0070 - val_acc: 0.9969\n",
      "Epoch 14/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0089 - val_acc: 0.9955\n",
      "Epoch 15/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0160 - val_acc: 0.9947\n",
      "Epoch 16/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0174 - acc: 0.9942 - val_loss: 0.0177 - val_acc: 0.9942\n",
      "Epoch 17/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0115 - acc: 0.9961 - val_loss: 0.0069 - val_acc: 0.9960\n",
      "Epoch 18/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0118 - acc: 0.9958 - val_loss: 0.0094 - val_acc: 0.9964\n",
      "Epoch 19/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.0093 - val_acc: 0.9960\n",
      "Epoch 20/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0053 - acc: 0.9979 - val_loss: 0.0071 - val_acc: 0.9964\n",
      "Epoch 21/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0090 - val_acc: 0.9955\n",
      "Epoch 22/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0061 - acc: 0.9976 - val_loss: 0.0101 - val_acc: 0.9955\n",
      "Epoch 23/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0052 - acc: 0.9983 - val_loss: 0.0093 - val_acc: 0.9955\n",
      "Epoch 24/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0076 - acc: 0.9972 - val_loss: 0.0140 - val_acc: 0.9951\n",
      "Epoch 25/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0096 - acc: 0.9973 - val_loss: 0.0152 - val_acc: 0.9951\n",
      "Epoch 26/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0125 - acc: 0.9955 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 27/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0145 - val_acc: 0.9938\n",
      "Epoch 28/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0071 - acc: 0.9974 - val_loss: 0.0077 - val_acc: 0.9973\n",
      "Epoch 29/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0085 - val_acc: 0.9964\n",
      "Epoch 30/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0064 - acc: 0.9978 - val_loss: 0.0081 - val_acc: 0.9964\n",
      "Epoch 31/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0062 - acc: 0.9978 - val_loss: 0.0135 - val_acc: 0.9942\n",
      "Epoch 32/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0090 - acc: 0.9966 - val_loss: 0.0221 - val_acc: 0.9920\n",
      "Epoch 33/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0151 - acc: 0.9945 - val_loss: 0.0186 - val_acc: 0.9915\n",
      "Epoch 34/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0110 - acc: 0.9964 - val_loss: 0.0093 - val_acc: 0.9960\n",
      "Epoch 35/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0146 - val_acc: 0.9933\n",
      "Epoch 36/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0056 - acc: 0.9979 - val_loss: 0.0096 - val_acc: 0.9947\n",
      "Epoch 37/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0045 - acc: 0.9982 - val_loss: 0.0119 - val_acc: 0.9955\n",
      "Epoch 38/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0095 - val_acc: 0.9951\n",
      "Epoch 39/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0106 - val_acc: 0.9960\n",
      "Epoch 40/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0065 - acc: 0.9974 - val_loss: 0.0155 - val_acc: 0.9960: 0s - loss: 0.0063 - acc: 0.997\n",
      "Epoch 41/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0124 - acc: 0.9957 - val_loss: 0.0146 - val_acc: 0.9933\n",
      "Epoch 42/20000\n",
      "135/135 [==============================] - 1s 8ms/step - loss: 0.0139 - acc: 0.9955 - val_loss: 0.0172 - val_acc: 0.9929\n",
      "Epoch 43/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0109 - acc: 0.9966 - val_loss: 0.0111 - val_acc: 0.9964\n",
      "Epoch 44/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9986 - val_loss: 0.0108 - val_acc: 0.9960\n",
      "Epoch 45/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0039 - acc: 0.9986 - val_loss: 0.0091 - val_acc: 0.9960\n",
      "Epoch 46/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0048 - acc: 0.9983 - val_loss: 0.0114 - val_acc: 0.9955\n",
      "Epoch 47/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0058 - acc: 0.9977 - val_loss: 0.0097 - val_acc: 0.9955\n",
      "Epoch 48/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0115 - val_acc: 0.9942\n",
      "Epoch 49/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0039 - acc: 0.9986 - val_loss: 0.0109 - val_acc: 0.9951\n",
      "Epoch 50/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0140 - val_acc: 0.9938\n",
      "Epoch 51/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0053 - acc: 0.9980 - val_loss: 0.0176 - val_acc: 0.9947\n",
      "Epoch 52/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0156 - val_acc: 0.9933\n",
      "Epoch 53/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0097 - acc: 0.9965 - val_loss: 0.0258 - val_acc: 0.9951\n",
      "Epoch 54/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0076 - acc: 0.9970 - val_loss: 0.0225 - val_acc: 0.9929\n",
      "Epoch 55/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0092 - acc: 0.9966 - val_loss: 0.0196 - val_acc: 0.9920\n",
      "Epoch 56/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0086 - acc: 0.9969 - val_loss: 0.0121 - val_acc: 0.9951\n",
      "Epoch 57/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0139 - acc: 0.9957 - val_loss: 0.0286 - val_acc: 0.9924\n",
      "Epoch 58/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0071 - acc: 0.9974 - val_loss: 0.0188 - val_acc: 0.9951\n",
      "Epoch 59/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0089 - acc: 0.9968 - val_loss: 0.0162 - val_acc: 0.9924\n",
      "Epoch 60/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0076 - acc: 0.9969 - val_loss: 0.0141 - val_acc: 0.9942\n",
      "Epoch 61/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0105 - val_acc: 0.9955\n",
      "Epoch 62/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.0132 - val_acc: 0.9964\n",
      "Epoch 63/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0130 - val_acc: 0.9947\n",
      "Epoch 64/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0181 - val_acc: 0.9938\n",
      "Epoch 65/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0099 - acc: 0.9966 - val_loss: 0.0138 - val_acc: 0.9951\n",
      "Epoch 66/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0180 - val_acc: 0.9933\n",
      "Epoch 67/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0137 - val_acc: 0.9947\n",
      "Epoch 68/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0054 - acc: 0.9979 - val_loss: 0.0106 - val_acc: 0.9951\n",
      "Epoch 69/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 70/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0202 - val_acc: 0.9929\n",
      "Epoch 71/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0049 - acc: 0.9980 - val_loss: 0.0148 - val_acc: 0.9947\n",
      "Epoch 72/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0103 - val_acc: 0.9960\n",
      "Epoch 73/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0088 - acc: 0.9969 - val_loss: 0.0192 - val_acc: 0.9924\n",
      "Epoch 74/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0113 - val_acc: 0.9942\n",
      "Epoch 75/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0105 - val_acc: 0.9960\n",
      "Epoch 76/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.0100 - val_acc: 0.9960\n",
      "Epoch 77/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0122 - val_acc: 0.9947\n",
      "Epoch 78/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0211 - val_acc: 0.9924\n",
      "Epoch 79/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0048 - acc: 0.9981 - val_loss: 0.0169 - val_acc: 0.9924\n",
      "Epoch 80/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0415 - val_acc: 0.9884\n",
      "Epoch 81/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0172 - acc: 0.9942 - val_loss: 0.0245 - val_acc: 0.9902\n",
      "Epoch 82/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0095 - acc: 0.9963 - val_loss: 0.0202 - val_acc: 0.9920\n",
      "Epoch 83/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0067 - acc: 0.9974 - val_loss: 0.0135 - val_acc: 0.9933\n",
      "Epoch 84/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0131 - val_acc: 0.9947\n",
      "Epoch 85/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0033 - acc: 0.9988 - val_loss: 0.0092 - val_acc: 0.9964\n",
      "Epoch 86/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0158 - val_acc: 0.9929\n",
      "Epoch 87/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0037 - acc: 0.9986 - val_loss: 0.0105 - val_acc: 0.9964\n",
      "Epoch 88/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.0115 - val_acc: 0.9951\n",
      "Epoch 89/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0088 - val_acc: 0.9960\n",
      "Epoch 90/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0136 - val_acc: 0.9947\n",
      "Epoch 91/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 0.9985 - val_loss: 0.0114 - val_acc: 0.9951\n",
      "Epoch 92/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0236 - val_acc: 0.9911\n",
      "Epoch 93/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0062 - acc: 0.9979 - val_loss: 0.0231 - val_acc: 0.9911\n",
      "Epoch 94/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0235 - acc: 0.9918 - val_loss: 0.0360 - val_acc: 0.9907\n",
      "Epoch 95/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0089 - acc: 0.9969 - val_loss: 0.0182 - val_acc: 0.9951\n",
      "Epoch 96/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.0185 - val_acc: 0.9938\n",
      "Epoch 97/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0155 - val_acc: 0.9942\n",
      "Epoch 98/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0135 - val_acc: 0.9951\n",
      "Epoch 99/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0155 - val_acc: 0.9942\n",
      "Epoch 100/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0111 - acc: 0.9971 - val_loss: 0.0234 - val_acc: 0.9915\n",
      "Epoch 101/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0119 - acc: 0.9959 - val_loss: 0.0218 - val_acc: 0.9924\n",
      "Epoch 102/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0194 - val_acc: 0.9911\n",
      "Epoch 103/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0097 - acc: 0.9973 - val_loss: 0.0475 - val_acc: 0.9849\n",
      "Epoch 104/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0136 - acc: 0.9952 - val_loss: 0.0172 - val_acc: 0.9947\n",
      "Epoch 105/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0111 - val_acc: 0.9955\n",
      "Epoch 1/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0070 - val_acc: 0.9978\n",
      "Epoch 2/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0065 - acc: 0.9977 - val_loss: 0.0034 - val_acc: 0.9987\n",
      "Epoch 3/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0042 - acc: 0.9985 - val_loss: 0.0034 - val_acc: 0.9991\n",
      "Epoch 4/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0048 - acc: 0.9984 - val_loss: 0.0059 - val_acc: 0.9973\n",
      "Epoch 5/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0061 - acc: 0.9976 - val_loss: 0.0037 - val_acc: 0.9987\n",
      "Epoch 6/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0043 - val_acc: 0.9982\n",
      "Epoch 7/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0032 - val_acc: 0.9982\n",
      "Epoch 8/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0028 - val_acc: 0.9996\n",
      "Epoch 9/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0115 - val_acc: 0.9964\n",
      "Epoch 10/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0061 - acc: 0.9978 - val_loss: 0.0101 - val_acc: 0.9947\n",
      "Epoch 11/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0039 - acc: 0.9985 - val_loss: 0.0058 - val_acc: 0.9973\n",
      "Epoch 12/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0169 - val_acc: 0.9933\n",
      "Epoch 13/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.0079 - val_acc: 0.9969\n",
      "Epoch 14/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0055 - acc: 0.9980 - val_loss: 0.0179 - val_acc: 0.9955\n",
      "Epoch 15/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0115 - acc: 0.9958 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 16/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0174 - acc: 0.9948 - val_loss: 0.0146 - val_acc: 0.9938\n",
      "Epoch 17/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0052 - acc: 0.9979 - val_loss: 0.0049 - val_acc: 0.9987\n",
      "Epoch 18/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0033 - val_acc: 0.9996\n",
      "Epoch 19/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0055 - val_acc: 0.9978\n",
      "Epoch 20/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0042 - acc: 0.9984 - val_loss: 0.0029 - val_acc: 0.9987\n",
      "Epoch 21/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0067 - val_acc: 0.9969\n",
      "Epoch 22/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.0044 - val_acc: 0.9987\n",
      "Epoch 23/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0040 - val_acc: 0.9982\n",
      "Epoch 24/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0092 - val_acc: 0.9969\n",
      "Epoch 25/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0077 - val_acc: 0.9969\n",
      "Epoch 26/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0060 - acc: 0.9978 - val_loss: 0.0302 - val_acc: 0.9929\n",
      "Epoch 27/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0115 - acc: 0.9960 - val_loss: 0.0039 - val_acc: 0.9996\n",
      "Epoch 28/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0080 - acc: 0.9969 - val_loss: 0.0106 - val_acc: 0.9955\n",
      "Epoch 29/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0061 - acc: 0.9983 - val_loss: 0.0059 - val_acc: 0.9978\n",
      "Epoch 30/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0038 - val_acc: 0.9982\n",
      "Epoch 31/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0040 - val_acc: 0.9987\n",
      "Epoch 32/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0050 - val_acc: 0.9982\n",
      "Epoch 33/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0026 - val_acc: 0.9991\n",
      "Epoch 34/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 0.0112 - val_acc: 0.9942\n",
      "Epoch 35/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0048 - acc: 0.9982 - val_loss: 0.0104 - val_acc: 0.9964\n",
      "Epoch 36/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0057 - val_acc: 0.9982\n",
      "Epoch 37/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0068 - acc: 0.9971 - val_loss: 0.0111 - val_acc: 0.9947\n",
      "Epoch 38/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0088 - acc: 0.9968 - val_loss: 0.0192 - val_acc: 0.9938\n",
      "Epoch 39/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0084 - acc: 0.9968 - val_loss: 0.0128 - val_acc: 0.9933\n",
      "Epoch 40/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0084 - acc: 0.9965 - val_loss: 0.0110 - val_acc: 0.9951\n",
      "Epoch 41/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0072 - val_acc: 0.9964\n",
      "Epoch 42/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0056 - val_acc: 0.9973\n",
      "Epoch 43/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0039 - val_acc: 0.9987\n",
      "Epoch 44/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0042 - val_acc: 0.9982\n",
      "Epoch 45/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0152 - val_acc: 0.9938\n",
      "Epoch 46/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0112 - val_acc: 0.9964\n",
      "Epoch 47/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 0.0102 - val_acc: 0.9960\n",
      "Epoch 48/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0065 - val_acc: 0.9969\n",
      "Epoch 49/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0043 - acc: 0.9984 - val_loss: 0.0125 - val_acc: 0.9947\n",
      "Epoch 50/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.0164 - val_acc: 0.9933\n",
      "Epoch 51/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0076 - acc: 0.9973 - val_loss: 0.0135 - val_acc: 0.9951\n",
      "Epoch 52/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0134 - acc: 0.9957 - val_loss: 0.0113 - val_acc: 0.9942\n",
      "Epoch 53/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0117 - val_acc: 0.9951\n",
      "Epoch 54/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0093 - val_acc: 0.9951\n",
      "Epoch 55/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0108 - val_acc: 0.9955\n",
      "Epoch 56/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9986 - val_loss: 0.0084 - val_acc: 0.9964\n",
      "Epoch 57/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0136 - val_acc: 0.9955\n",
      "Epoch 58/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0062 - val_acc: 0.9969\n",
      "Epoch 59/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0091 - val_acc: 0.9955\n",
      "Epoch 60/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0054 - val_acc: 0.9969\n",
      "Epoch 61/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 0.0064 - val_acc: 0.9969\n",
      "Epoch 62/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0112 - val_acc: 0.9951\n",
      "Epoch 63/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0132 - acc: 0.9958 - val_loss: 0.0200 - val_acc: 0.9920\n",
      "Epoch 64/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.0145 - val_acc: 0.9955\n",
      "Epoch 65/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9986 - val_loss: 0.0096 - val_acc: 0.9969\n",
      "Epoch 66/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0049 - val_acc: 0.9982\n",
      "Epoch 67/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.0092 - val_acc: 0.9960\n",
      "Epoch 68/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0116 - val_acc: 0.9955\n",
      "Epoch 69/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0114 - val_acc: 0.9955\n",
      "Epoch 70/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0259 - val_acc: 0.9915\n",
      "Epoch 71/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0182 - val_acc: 0.9942\n",
      "Epoch 72/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0077 - val_acc: 0.9969\n",
      "Epoch 73/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0133 - val_acc: 0.9955\n",
      "Epoch 74/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0111 - val_acc: 0.9960\n",
      "Epoch 75/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0087 - val_acc: 0.9973\n",
      "Epoch 76/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.0074 - val_acc: 0.9964\n",
      "Epoch 77/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0072 - acc: 0.9975 - val_loss: 0.0125 - val_acc: 0.9960\n",
      "Epoch 78/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0158 - acc: 0.9946 - val_loss: 0.0145 - val_acc: 0.9938\n",
      "Epoch 79/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0104 - acc: 0.9969 - val_loss: 0.0132 - val_acc: 0.9951\n",
      "Epoch 80/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 0.0080 - val_acc: 0.9955\n",
      "Epoch 81/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0081 - val_acc: 0.9973\n",
      "Epoch 82/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.0095 - val_acc: 0.9955\n",
      "Epoch 83/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0080 - val_acc: 0.9964\n",
      "Epoch 84/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0074 - val_acc: 0.9960\n",
      "Epoch 85/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0071 - val_acc: 0.9964\n",
      "Epoch 86/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0093 - val_acc: 0.9955\n",
      "Epoch 87/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0087 - val_acc: 0.9955\n",
      "Epoch 88/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0081 - val_acc: 0.9964\n",
      "Epoch 89/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0109 - val_acc: 0.9947\n",
      "Epoch 90/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0055 - acc: 0.9980 - val_loss: 0.0286 - val_acc: 0.9924\n",
      "Epoch 91/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0176 - acc: 0.9946 - val_loss: 0.0285 - val_acc: 0.9902\n",
      "Epoch 92/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0156 - val_acc: 0.9942\n",
      "Epoch 93/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0066 - val_acc: 0.9978\n",
      "Epoch 94/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.0054 - val_acc: 0.9978\n",
      "Epoch 95/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0073 - val_acc: 0.9969\n",
      "Epoch 96/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0071 - val_acc: 0.9982\n",
      "Epoch 97/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0076 - val_acc: 0.9969\n",
      "Epoch 98/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9992 - val_loss: 0.0074 - val_acc: 0.9969\n",
      "Epoch 99/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0185 - val_acc: 0.9938\n",
      "Epoch 100/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9987 - val_loss: 0.0196 - val_acc: 0.9951\n",
      "Epoch 101/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0212 - acc: 0.9942 - val_loss: 0.0257 - val_acc: 0.9915\n",
      "Epoch 102/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.0163 - val_acc: 0.9960\n",
      "Epoch 103/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0095 - val_acc: 0.9964\n",
      "Epoch 104/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0097 - val_acc: 0.9955\n",
      "Epoch 105/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0095 - val_acc: 0.9960\n",
      "Epoch 106/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0087 - val_acc: 0.9960\n",
      "Epoch 107/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0151 - val_acc: 0.9960\n",
      "Epoch 108/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0296 - val_acc: 0.9911\n",
      "Epoch 109/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 0.0105 - val_acc: 0.9955\n",
      "Epoch 110/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0085 - val_acc: 0.9955\n",
      "Epoch 111/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0116 - val_acc: 0.9960\n",
      "Epoch 112/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.7949e-04 - acc: 0.9998 - val_loss: 0.0085 - val_acc: 0.9960\n",
      "Epoch 113/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.2051e-04 - acc: 0.9998 - val_loss: 0.0135 - val_acc: 0.9947\n",
      "Epoch 114/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0125 - acc: 0.9957 - val_loss: 0.0381 - val_acc: 0.9924\n",
      "Epoch 115/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0107 - acc: 0.9959 - val_loss: 0.0159 - val_acc: 0.9938\n",
      "Epoch 116/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0056 - acc: 0.9976 - val_loss: 0.0125 - val_acc: 0.9955\n",
      "Epoch 117/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0150 - val_acc: 0.9947\n",
      "Epoch 118/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 0.0083 - val_acc: 0.9969\n",
      "Epoch 119/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0081 - val_acc: 0.9978\n",
      "Epoch 120/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.0110 - val_acc: 0.9960\n",
      "Epoch 121/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.3495e-04 - acc: 0.9999 - val_loss: 0.0085 - val_acc: 0.9973\n",
      "Epoch 122/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.0374e-04 - acc: 0.9998 - val_loss: 0.0085 - val_acc: 0.9973\n",
      "Epoch 123/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0199 - val_acc: 0.9933\n",
      "Epoch 124/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0111 - val_acc: 0.9964\n",
      "Epoch 125/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9992 - val_loss: 0.0117 - val_acc: 0.9960\n",
      "Epoch 126/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0195 - val_acc: 0.9938\n",
      "Epoch 127/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0253 - val_acc: 0.9924\n",
      "Epoch 128/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0062 - acc: 0.9979 - val_loss: 0.0387 - val_acc: 0.9862\n",
      "Epoch 129/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0173 - acc: 0.9944 - val_loss: 0.0400 - val_acc: 0.9898\n",
      "Epoch 130/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0097 - acc: 0.9973 - val_loss: 0.0191 - val_acc: 0.9915\n",
      "Epoch 131/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0096 - val_acc: 0.9947\n",
      "Epoch 132/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.8009e-04 - acc: 0.9998 - val_loss: 0.0134 - val_acc: 0.9938\n",
      "Epoch 133/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.0116 - val_acc: 0.9942\n",
      "Epoch 1/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0058 - acc: 0.9978 - val_loss: 0.0067 - val_acc: 0.9973\n",
      "Epoch 2/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0075 - acc: 0.9975 - val_loss: 0.0026 - val_acc: 0.9991\n",
      "Epoch 3/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0038 - val_acc: 0.9982\n",
      "Epoch 4/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0022 - val_acc: 0.9991\n",
      "Epoch 5/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.0027 - val_acc: 0.9982\n",
      "Epoch 6/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9992 - val_loss: 0.0022 - val_acc: 0.9991\n",
      "Epoch 7/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0129 - val_acc: 0.9960\n",
      "Epoch 8/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0053 - val_acc: 0.9978\n",
      "Epoch 9/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0132 - acc: 0.9966 - val_loss: 0.0054 - val_acc: 0.9973\n",
      "Epoch 10/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0039 - val_acc: 0.9982\n",
      "Epoch 11/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0027 - val_acc: 0.9987\n",
      "Epoch 12/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0033 - val_acc: 0.9991\n",
      "Epoch 13/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 8.9542e-04 - acc: 0.9998 - val_loss: 0.0047 - val_acc: 0.9973\n",
      "Epoch 14/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0030 - val_acc: 0.9987\n",
      "Epoch 15/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0077 - val_acc: 0.9969\n",
      "Epoch 16/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0096 - acc: 0.9967 - val_loss: 0.0081 - val_acc: 0.9960\n",
      "Epoch 17/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0049 - acc: 0.9980 - val_loss: 0.0117 - val_acc: 0.9969\n",
      "Epoch 18/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0065 - acc: 0.9975 - val_loss: 0.0076 - val_acc: 0.9969\n",
      "Epoch 19/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0044 - acc: 0.9982 - val_loss: 0.0089 - val_acc: 0.9964\n",
      "Epoch 20/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0033 - val_acc: 0.9987\n",
      "Epoch 21/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0033 - val_acc: 0.9987\n",
      "Epoch 22/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.6430e-04 - acc: 0.9997 - val_loss: 0.0026 - val_acc: 0.9991\n",
      "Epoch 23/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.4388e-04 - acc: 0.9998 - val_loss: 0.0029 - val_acc: 0.9991\n",
      "Epoch 24/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.9206e-04 - acc: 0.9999 - val_loss: 0.0023 - val_acc: 0.9996\n",
      "Epoch 25/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 6.8250e-04 - acc: 0.9998 - val_loss: 0.0024 - val_acc: 0.9991\n",
      "Epoch 26/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 8.1632e-04 - acc: 0.9997 - val_loss: 0.0035 - val_acc: 0.9982\n",
      "Epoch 27/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.6476e-04 - acc: 0.9999 - val_loss: 0.0076 - val_acc: 0.9969\n",
      "Epoch 28/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0021 - val_acc: 0.9987\n",
      "Epoch 29/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0101 - acc: 0.9971 - val_loss: 0.0585 - val_acc: 0.9871\n",
      "Epoch 30/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0125 - acc: 0.9956 - val_loss: 0.0121 - val_acc: 0.9960\n",
      "Epoch 31/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0074 - acc: 0.9979 - val_loss: 0.0096 - val_acc: 0.9978\n",
      "Epoch 32/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0117 - val_acc: 0.9951\n",
      "Epoch 33/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0056 - val_acc: 0.9978\n",
      "Epoch 34/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0074 - val_acc: 0.9982\n",
      "Epoch 35/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.2254e-04 - acc: 0.9998 - val_loss: 0.0057 - val_acc: 0.9978\n",
      "Epoch 36/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 0.0106 - val_acc: 0.9964\n",
      "Epoch 37/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0050 - val_acc: 0.9991\n",
      "Epoch 38/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 9.5375e-04 - acc: 0.9997 - val_loss: 0.0057 - val_acc: 0.9982\n",
      "Epoch 39/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 8.5285e-04 - acc: 0.9999 - val_loss: 0.0034 - val_acc: 0.9987\n",
      "Epoch 40/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.4205e-04 - acc: 0.9996 - val_loss: 0.0070 - val_acc: 0.9973\n",
      "Epoch 41/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.9314e-04 - acc: 0.9999 - val_loss: 0.0050 - val_acc: 0.9982\n",
      "Epoch 42/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.5713e-04 - acc: 0.9999 - val_loss: 0.0045 - val_acc: 0.9978\n",
      "Epoch 43/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.2258e-04 - acc: 0.9999 - val_loss: 0.0038 - val_acc: 0.9982\n",
      "Epoch 44/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.4657e-04 - acc: 0.9999 - val_loss: 0.0047 - val_acc: 0.9978\n",
      "Epoch 45/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.5141e-04 - acc: 0.9998 - val_loss: 0.0048 - val_acc: 0.9982\n",
      "Epoch 46/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0195 - acc: 0.9946 - val_loss: 0.0317 - val_acc: 0.9898\n",
      "Epoch 47/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0166 - acc: 0.9953 - val_loss: 0.0433 - val_acc: 0.9951\n",
      "Epoch 48/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9985 - val_loss: 0.0205 - val_acc: 0.9955\n",
      "Epoch 49/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0175 - val_acc: 0.9978\n",
      "Epoch 50/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0180 - val_acc: 0.9978\n",
      "Epoch 51/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.9577e-04 - acc: 0.9998 - val_loss: 0.0190 - val_acc: 0.9982\n",
      "Epoch 52/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.1436e-04 - acc: 0.9999 - val_loss: 0.0207 - val_acc: 0.9973\n",
      "Epoch 53/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.0488e-04 - acc: 0.9999 - val_loss: 0.0205 - val_acc: 0.9982\n",
      "Epoch 54/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 5.1341e-04 - acc: 1.0000 - val_loss: 0.0234 - val_acc: 0.9978\n",
      "Epoch 55/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.0652e-04 - acc: 0.9998 - val_loss: 0.0245 - val_acc: 0.9982\n",
      "Epoch 56/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.1081e-04 - acc: 0.9999 - val_loss: 0.0280 - val_acc: 0.9978\n",
      "Epoch 57/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.8088e-04 - acc: 1.0000 - val_loss: 0.0286 - val_acc: 0.9955\n",
      "Epoch 58/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.9201e-04 - acc: 0.9999 - val_loss: 0.0277 - val_acc: 0.9969\n",
      "Epoch 59/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.8898e-04 - acc: 0.9999 - val_loss: 0.0277 - val_acc: 0.9978\n",
      "Epoch 60/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0690 - val_acc: 0.9920\n",
      "Epoch 61/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0157 - acc: 0.9953 - val_loss: 0.0103 - val_acc: 0.9955\n",
      "Epoch 62/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0087 - val_acc: 0.9960\n",
      "Epoch 63/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0099 - val_acc: 0.9960\n",
      "Epoch 64/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0052 - val_acc: 0.9987\n",
      "Epoch 65/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.2240e-04 - acc: 0.9998 - val_loss: 0.0048 - val_acc: 0.9964\n",
      "Epoch 66/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.1563e-04 - acc: 0.9999 - val_loss: 0.0030 - val_acc: 0.9982\n",
      "Epoch 67/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.9571e-04 - acc: 0.9999 - val_loss: 0.0064 - val_acc: 0.9964\n",
      "Epoch 68/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0045 - val_acc: 0.9987\n",
      "Epoch 69/20000\n",
      "135/135 [==============================] - 1s 7ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0081 - val_acc: 0.9969\n",
      "Epoch 70/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.3351e-04 - acc: 0.9998 - val_loss: 0.0056 - val_acc: 0.9978\n",
      "Epoch 71/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0096 - val_acc: 0.9964\n",
      "Epoch 72/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0072 - val_acc: 0.9964\n",
      "Epoch 73/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0119 - val_acc: 0.9960\n",
      "Epoch 74/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.9876e-04 - acc: 0.9998 - val_loss: 0.0054 - val_acc: 0.9964\n",
      "Epoch 75/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.7271e-04 - acc: 0.9999 - val_loss: 0.0064 - val_acc: 0.9960\n",
      "Epoch 76/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 4.2180e-04 - acc: 0.9999 - val_loss: 0.0063 - val_acc: 0.9969\n",
      "Epoch 77/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.5675e-04 - acc: 0.9999 - val_loss: 0.0073 - val_acc: 0.9964\n",
      "Epoch 78/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0130 - acc: 0.9961 - val_loss: 0.0450 - val_acc: 0.9911\n",
      "Epoch 79/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0155 - acc: 0.9947 - val_loss: 0.0179 - val_acc: 0.9938\n",
      "Epoch 80/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0131 - val_acc: 0.9938\n",
      "Epoch 81/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.0168 - val_acc: 0.9955\n",
      "Epoch 82/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0181 - val_acc: 0.9964\n",
      "Epoch 83/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0125 - val_acc: 0.9955\n",
      "Epoch 84/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.8238e-04 - acc: 0.9999 - val_loss: 0.0116 - val_acc: 0.9969\n",
      "Epoch 85/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.2970e-04 - acc: 0.9999 - val_loss: 0.0122 - val_acc: 0.9969\n",
      "Epoch 86/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.1734e-04 - acc: 1.0000 - val_loss: 0.0125 - val_acc: 0.9964\n",
      "Epoch 87/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.0590e-04 - acc: 0.9999 - val_loss: 0.0118 - val_acc: 0.9969\n",
      "Epoch 88/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 6.2276e-04 - acc: 0.9998 - val_loss: 0.0121 - val_acc: 0.9969\n",
      "Epoch 89/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0154 - val_acc: 0.9960\n",
      "Epoch 90/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.0320 - val_acc: 0.9929\n",
      "Epoch 91/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0197 - val_acc: 0.9915\n",
      "Epoch 92/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0255 - val_acc: 0.9924\n",
      "Epoch 93/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0186 - val_acc: 0.9964\n",
      "Epoch 94/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.6736e-04 - acc: 0.9998 - val_loss: 0.0160 - val_acc: 0.9960\n",
      "Epoch 95/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 5.6712e-04 - acc: 1.0000 - val_loss: 0.0149 - val_acc: 0.9960\n",
      "Epoch 96/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 5.6659e-04 - acc: 0.9999 - val_loss: 0.0148 - val_acc: 0.9969\n",
      "Epoch 97/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.4850e-04 - acc: 0.9998 - val_loss: 0.0152 - val_acc: 0.9964\n",
      "Epoch 98/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.0575e-04 - acc: 1.0000 - val_loss: 0.0131 - val_acc: 0.9964\n",
      "Epoch 99/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.0115e-04 - acc: 0.9999 - val_loss: 0.0132 - val_acc: 0.9969\n",
      "Epoch 100/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.4766e-04 - acc: 1.0000 - val_loss: 0.0150 - val_acc: 0.9955\n",
      "Epoch 101/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0297 - val_acc: 0.9924\n",
      "Epoch 102/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0063 - acc: 0.9977 - val_loss: 0.0504 - val_acc: 0.9898\n",
      "Epoch 103/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0242 - val_acc: 0.9933\n",
      "Epoch 104/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0061 - acc: 0.9976 - val_loss: 0.0087 - val_acc: 0.9973\n",
      "Epoch 105/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0076 - acc: 0.9977 - val_loss: 0.0225 - val_acc: 0.9947\n",
      "Epoch 106/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0050 - acc: 0.9987 - val_loss: 0.0089 - val_acc: 0.9960\n",
      "Epoch 107/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0093 - val_acc: 0.9969\n",
      "Epoch 108/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 6.7969e-04 - acc: 0.9999 - val_loss: 0.0078 - val_acc: 0.9969\n",
      "Epoch 109/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 4.8558e-04 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 0.9973\n",
      "Epoch 110/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.1125e-04 - acc: 1.0000 - val_loss: 0.0088 - val_acc: 0.9964\n",
      "Epoch 111/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 4.0434e-04 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9960\n",
      "Epoch 112/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 3.8689e-04 - acc: 0.9999 - val_loss: 0.0086 - val_acc: 0.9960\n",
      "Epoch 113/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.6841e-04 - acc: 0.9999 - val_loss: 0.0083 - val_acc: 0.9969\n",
      "Epoch 114/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.9207e-04 - acc: 0.9999 - val_loss: 0.0084 - val_acc: 0.9964\n",
      "Epoch 115/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 3.2408e-04 - acc: 1.0000 - val_loss: 0.0089 - val_acc: 0.9955\n",
      "Epoch 116/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 4.8995e-04 - acc: 0.9999 - val_loss: 0.0111 - val_acc: 0.9969\n",
      "Epoch 117/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0160 - val_acc: 0.9951\n",
      "Epoch 118/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0336 - val_acc: 0.9907\n",
      "Epoch 119/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 0.0127 - acc: 0.9956 - val_loss: 0.0239 - val_acc: 0.9924\n",
      "Epoch 120/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0181 - val_acc: 0.9947\n",
      "Epoch 121/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0154 - val_acc: 0.9942\n",
      "Epoch 122/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 5.9113e-04 - acc: 1.0000 - val_loss: 0.0168 - val_acc: 0.9938\n",
      "Epoch 123/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 5.1429e-04 - acc: 0.9999 - val_loss: 0.0160 - val_acc: 0.9947\n",
      "Epoch 124/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 3.9740e-04 - acc: 0.9999 - val_loss: 0.0158 - val_acc: 0.9933\n",
      "Epoch 125/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 5.3156e-04 - acc: 0.9999 - val_loss: 0.0183 - val_acc: 0.9938\n",
      "Epoch 126/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 3.6777e-04 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9942\n",
      "Epoch 127/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 4.1008e-04 - acc: 1.0000 - val_loss: 0.0181 - val_acc: 0.9924\n",
      "Epoch 128/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.6020e-04 - acc: 1.0000 - val_loss: 0.0169 - val_acc: 0.9933\n",
      "Epoch 1/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0110 - acc: 0.9966 - val_loss: 0.0143 - val_acc: 0.9942\n",
      "Epoch 2/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0074 - acc: 0.9971 - val_loss: 0.0026 - val_acc: 0.9991\n",
      "Epoch 3/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0035 - acc: 0.9986 - val_loss: 0.0021 - val_acc: 0.9996\n",
      "Epoch 4/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 5/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 6/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0035 - val_acc: 0.9987\n",
      "Epoch 7/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0046 - acc: 0.9982 - val_loss: 0.0085 - val_acc: 0.9973\n",
      "Epoch 8/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Epoch 9/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0028 - val_acc: 0.9991\n",
      "Epoch 10/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.0013 - val_acc: 0.9991\n",
      "Epoch 11/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 6.9625e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 12/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 8.3425e-04 - acc: 0.9999 - val_loss: 7.8947e-04 - val_acc: 1.0000\n",
      "Epoch 13/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.7165e-04 - acc: 0.9999 - val_loss: 0.0021 - val_acc: 0.9991\n",
      "Epoch 14/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 9.5569e-04 - acc: 0.9998 - val_loss: 0.0030 - val_acc: 0.9987\n",
      "Epoch 15/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0103 - acc: 0.9968 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 16/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0117 - acc: 0.9961 - val_loss: 0.0060 - val_acc: 0.9978\n",
      "Epoch 17/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0022 - val_acc: 0.9991\n",
      "Epoch 18/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Epoch 19/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 6.9825e-04 - acc: 0.9999 - val_loss: 0.0020 - val_acc: 0.9991\n",
      "Epoch 20/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.2211e-04 - acc: 0.9999 - val_loss: 0.0013 - val_acc: 0.9996\n",
      "Epoch 21/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.2348e-04 - acc: 0.9998 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 22/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.0148e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9991\n",
      "Epoch 23/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.6268e-04 - acc: 0.9999 - val_loss: 0.0016 - val_acc: 0.9991\n",
      "Epoch 24/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 4.2923e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9991\n",
      "Epoch 25/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.2496e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Epoch 26/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.5097e-04 - acc: 0.9999 - val_loss: 0.0016 - val_acc: 0.9991\n",
      "Epoch 27/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 4.3953e-04 - acc: 0.9999 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 28/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.6848e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Epoch 29/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0398 - val_acc: 0.9880\n",
      "Epoch 30/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0202 - acc: 0.9944 - val_loss: 0.0143 - val_acc: 0.9938\n",
      "Epoch 31/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0041 - acc: 0.9984 - val_loss: 0.0061 - val_acc: 0.9978\n",
      "Epoch 32/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0055 - val_acc: 0.9991\n",
      "Epoch 33/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 0.0054 - val_acc: 0.9978\n",
      "Epoch 34/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.5143e-04 - acc: 0.9998 - val_loss: 0.0026 - val_acc: 0.9987\n",
      "Epoch 35/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.1939e-04 - acc: 0.9999 - val_loss: 0.0019 - val_acc: 0.9991\n",
      "Epoch 36/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.7767e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9991\n",
      "Epoch 37/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 3.5273e-04 - acc: 0.9999 - val_loss: 0.0020 - val_acc: 0.9982\n",
      "Epoch 38/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.8893e-04 - acc: 0.9998 - val_loss: 0.0021 - val_acc: 0.9991\n",
      "Epoch 39/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 3.7519e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9991\n",
      "Epoch 40/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.1959e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Epoch 41/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.7223e-04 - acc: 0.9998 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Epoch 42/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0081 - val_acc: 0.9978\n",
      "Epoch 43/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0092 - acc: 0.9969 - val_loss: 0.0293 - val_acc: 0.9911\n",
      "Epoch 44/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0059 - acc: 0.9978 - val_loss: 0.0061 - val_acc: 0.9973\n",
      "Epoch 45/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0056 - acc: 0.9979 - val_loss: 0.0044 - val_acc: 0.9973\n",
      "Epoch 46/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 9.3587e-04 - acc: 0.9998 - val_loss: 0.0027 - val_acc: 0.9991\n",
      "Epoch 47/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.8133e-04 - acc: 0.9999 - val_loss: 0.0028 - val_acc: 0.9982\n",
      "Epoch 48/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0071 - val_acc: 0.9973\n",
      "Epoch 49/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0040 - val_acc: 0.9987\n",
      "Epoch 50/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0054 - val_acc: 0.9978\n",
      "Epoch 51/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0064 - val_acc: 0.9978\n",
      "Epoch 52/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0165 - val_acc: 0.9947\n",
      "Epoch 53/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0066 - acc: 0.9979 - val_loss: 0.0135 - val_acc: 0.9955\n",
      "Epoch 54/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0147 - val_acc: 0.9960\n",
      "Epoch 55/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0111 - val_acc: 0.9969\n",
      "Epoch 56/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0064 - val_acc: 0.9978\n",
      "Epoch 57/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.6691e-04 - acc: 0.9998 - val_loss: 0.0031 - val_acc: 0.9987\n",
      "Epoch 58/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.9360e-04 - acc: 0.9998 - val_loss: 0.0054 - val_acc: 0.9978\n",
      "Epoch 59/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.2687e-04 - acc: 0.9999 - val_loss: 0.0052 - val_acc: 0.9987\n",
      "Epoch 60/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.4946e-04 - acc: 0.9999 - val_loss: 0.0053 - val_acc: 0.9978\n",
      "Epoch 61/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.7459e-04 - acc: 0.9998 - val_loss: 0.0037 - val_acc: 0.9982\n",
      "Epoch 62/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.9314e-04 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9978\n",
      "Epoch 63/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5104e-04 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9982\n",
      "Epoch 64/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.8645e-04 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9987\n",
      "Epoch 65/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.9482e-04 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9978\n",
      "Epoch 66/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 3.0843e-04 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 0.9982\n",
      "Epoch 67/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0294 - val_acc: 0.9920\n",
      "Epoch 68/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0115 - acc: 0.9961 - val_loss: 0.0337 - val_acc: 0.9929\n",
      "Epoch 69/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 0.0069 - val_acc: 0.9973\n",
      "Epoch 70/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0054 - val_acc: 0.9973\n",
      "Epoch 71/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.9259e-04 - acc: 0.9999 - val_loss: 0.0032 - val_acc: 0.9991\n",
      "Epoch 72/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 3.2898e-04 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 0.9987\n",
      "Epoch 73/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 3.0798e-04 - acc: 0.9999 - val_loss: 0.0028 - val_acc: 0.9991\n",
      "Epoch 74/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 2.3711e-04 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 0.9991\n",
      "Epoch 75/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 2.3797e-04 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9982\n",
      "Epoch 76/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.7077e-04 - acc: 0.9999 - val_loss: 0.0033 - val_acc: 0.9987\n",
      "Epoch 77/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 4.7684e-04 - acc: 0.9999 - val_loss: 0.0073 - val_acc: 0.9978\n",
      "Epoch 78/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0116 - acc: 0.9961 - val_loss: 0.0173 - val_acc: 0.9933\n",
      "Epoch 79/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0071 - acc: 0.9971 - val_loss: 0.0150 - val_acc: 0.9955\n",
      "Epoch 80/20000\n",
      "135/135 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.999 - 0s 3ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 81/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.1590e-04 - acc: 0.9998 - val_loss: 0.0028 - val_acc: 0.9991\n",
      "Epoch 82/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0064 - val_acc: 0.9973\n",
      "Epoch 83/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0028 - val_acc: 0.9987\n",
      "Epoch 84/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.9704e-04 - acc: 0.9997 - val_loss: 0.0037 - val_acc: 0.9987\n",
      "Epoch 85/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.2560e-04 - acc: 0.9998 - val_loss: 0.0037 - val_acc: 0.9978\n",
      "Epoch 86/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0072 - val_acc: 0.9964\n",
      "Epoch 87/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0112 - val_acc: 0.9973\n",
      "Epoch 88/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0056 - val_acc: 0.9982\n",
      "Epoch 89/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0298 - val_acc: 0.9933\n",
      "Epoch 90/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0062 - acc: 0.9986 - val_loss: 0.0054 - val_acc: 0.9978\n",
      "Epoch 91/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0050 - val_acc: 0.9987\n",
      "Epoch 92/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.0428e-04 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 0.9987\n",
      "Epoch 93/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.8542e-04 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 0.9982\n",
      "Epoch 94/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.5013e-04 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 0.9982\n",
      "Epoch 95/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.9531e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9982\n",
      "Epoch 96/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.6538e-04 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9982\n",
      "Epoch 97/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.7729e-04 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 0.9978\n",
      "Epoch 98/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.3848e-04 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9982\n",
      "Epoch 99/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.8833e-04 - acc: 0.9998 - val_loss: 0.0074 - val_acc: 0.9987\n",
      "Epoch 100/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.1343e-04 - acc: 0.9998 - val_loss: 0.0060 - val_acc: 0.9982\n",
      "Epoch 101/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0128 - acc: 0.9956 - val_loss: 0.0402 - val_acc: 0.9898\n",
      "Epoch 102/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0126 - acc: 0.9968 - val_loss: 0.0093 - val_acc: 0.9973\n",
      "Epoch 103/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "Epoch 104/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.2471e-04 - acc: 0.9999 - val_loss: 0.0090 - val_acc: 0.9973\n",
      "Epoch 105/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.6806e-04 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 0.9964\n",
      "Epoch 106/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.0250e-04 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 0.9969\n",
      "Epoch 107/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.7928e-04 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 0.9960\n",
      "Epoch 108/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.4844e-04 - acc: 1.0000 - val_loss: 0.0093 - val_acc: 0.9964\n",
      "Epoch 109/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5918e-04 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 0.9964\n",
      "Epoch 110/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.0990e-04 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 0.9960\n",
      "Epoch 111/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.9672e-04 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 0.9964\n",
      "Epoch 112/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.3518e-04 - acc: 0.9999 - val_loss: 0.0101 - val_acc: 0.9964\n",
      "Epoch 1/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0075 - val_acc: 0.9982\n",
      "Epoch 2/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0096 - acc: 0.9969 - val_loss: 0.0126 - val_acc: 0.9964\n",
      "Epoch 3/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0056 - val_acc: 0.9982\n",
      "Epoch 4/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 5/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0045 - val_acc: 0.9987\n",
      "Epoch 6/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0037 - val_acc: 0.9987\n",
      "Epoch 7/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0038 - val_acc: 0.9987\n",
      "Epoch 8/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.2044e-04 - acc: 0.9998 - val_loss: 0.0022 - val_acc: 0.9996\n",
      "Epoch 9/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.9598e-04 - acc: 0.9999 - val_loss: 0.0022 - val_acc: 0.9996\n",
      "Epoch 10/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 4.1265e-04 - acc: 0.9999 - val_loss: 0.0023 - val_acc: 0.9991\n",
      "Epoch 11/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.7808e-04 - acc: 0.9998 - val_loss: 0.0021 - val_acc: 0.9996\n",
      "Epoch 12/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.5190e-04 - acc: 0.9999 - val_loss: 0.0023 - val_acc: 0.9991\n",
      "Epoch 13/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.8390e-04 - acc: 0.9999 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Epoch 14/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.9481e-04 - acc: 0.9998 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Epoch 15/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0010 - acc: 0.9995 - val_loss: 0.0069 - val_acc: 0.9978\n",
      "Epoch 16/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0064 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 0.9987\n",
      "Epoch 17/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0067 - val_acc: 0.9987\n",
      "Epoch 18/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0054 - acc: 0.9985 - val_loss: 0.0055 - val_acc: 0.9982\n",
      "Epoch 19/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0023 - val_acc: 0.9987\n",
      "Epoch 20/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0039 - val_acc: 0.9987\n",
      "Epoch 21/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 7.3251e-04 - val_acc: 1.0000\n",
      "Epoch 22/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.6104e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9991\n",
      "Epoch 23/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.2721e-04 - acc: 0.9999 - val_loss: 7.2556e-04 - val_acc: 1.0000\n",
      "Epoch 24/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.5594e-04 - acc: 0.9998 - val_loss: 0.0013 - val_acc: 0.9996\n",
      "Epoch 25/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.8661e-04 - acc: 0.9999 - val_loss: 9.0943e-04 - val_acc: 0.9996\n",
      "Epoch 26/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 27/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0092 - val_acc: 0.9960\n",
      "Epoch 28/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.0077 - val_acc: 0.9982\n",
      "Epoch 29/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0022 - val_acc: 0.9991\n",
      "Epoch 30/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 7.4792e-04 - val_acc: 1.0000\n",
      "Epoch 31/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.4371e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9991\n",
      "Epoch 32/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.3312e-04 - acc: 0.9999 - val_loss: 0.0020 - val_acc: 0.9991\n",
      "Epoch 33/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.2206e-04 - acc: 0.9998 - val_loss: 0.0040 - val_acc: 0.9991\n",
      "Epoch 34/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.2381e-04 - acc: 0.9999 - val_loss: 0.0018 - val_acc: 0.9991\n",
      "Epoch 35/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.2402e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9991\n",
      "Epoch 36/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3576e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 37/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.4136e-04 - acc: 0.9998 - val_loss: 0.0014 - val_acc: 0.9991\n",
      "Epoch 38/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.7783e-04 - acc: 0.9999 - val_loss: 0.0018 - val_acc: 0.9991\n",
      "Epoch 39/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0024 - acc: 0.9988 - val_loss: 0.0263 - val_acc: 0.9938\n",
      "Epoch 40/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0091 - acc: 0.9973 - val_loss: 0.0037 - val_acc: 0.9982\n",
      "Epoch 41/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.0043 - val_acc: 0.9982\n",
      "Epoch 42/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0043 - val_acc: 0.9991\n",
      "Epoch 43/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0120 - val_acc: 0.9973\n",
      "Epoch 44/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0155 - val_acc: 0.9947\n",
      "Epoch 45/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0035 - val_acc: 0.9982\n",
      "Epoch 46/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.1986e-04 - acc: 0.9999 - val_loss: 0.0025 - val_acc: 0.9991\n",
      "Epoch 47/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.3144e-04 - acc: 0.9998 - val_loss: 0.0048 - val_acc: 0.9987\n",
      "Epoch 48/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.1463e-04 - acc: 0.9998 - val_loss: 0.0049 - val_acc: 0.9978\n",
      "Epoch 49/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0045 - val_acc: 0.9982\n",
      "Epoch 50/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.9409e-04 - acc: 0.9999 - val_loss: 0.0049 - val_acc: 0.9991\n",
      "Epoch 51/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.0420e-04 - acc: 0.9998 - val_loss: 0.0062 - val_acc: 0.9987\n",
      "Epoch 52/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0040 - val_acc: 0.9987\n",
      "Epoch 53/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 7.2871e-04 - acc: 0.9998 - val_loss: 0.0034 - val_acc: 0.9987\n",
      "Epoch 54/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0098 - acc: 0.9972 - val_loss: 0.0131 - val_acc: 0.9964\n",
      "Epoch 55/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.0045 - val_acc: 0.9987\n",
      "Epoch 56/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.6130e-04 - acc: 0.9998 - val_loss: 0.0032 - val_acc: 0.9987\n",
      "Epoch 57/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.9429e-04 - acc: 0.9999 - val_loss: 0.0020 - val_acc: 0.9987\n",
      "Epoch 58/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.4884e-04 - acc: 0.9999 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Epoch 59/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 3.4220e-04 - acc: 0.9999 - val_loss: 0.0028 - val_acc: 0.9987\n",
      "Epoch 60/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.6541e-04 - acc: 0.9999 - val_loss: 0.0020 - val_acc: 0.9991\n",
      "Epoch 61/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.0791e-04 - acc: 0.9999 - val_loss: 0.0020 - val_acc: 0.9991\n",
      "Epoch 62/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.7677e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Epoch 63/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0050 - val_acc: 0.9982\n",
      "Epoch 64/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0062 - acc: 0.9983 - val_loss: 0.0058 - val_acc: 0.9987\n",
      "Epoch 65/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0169 - val_acc: 0.9951\n",
      "Epoch 66/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0074 - acc: 0.9974 - val_loss: 0.0078 - val_acc: 0.9973\n",
      "Epoch 67/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0055 - val_acc: 0.9982\n",
      "Epoch 68/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.1421e-04 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9987\n",
      "Epoch 69/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.3454e-04 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9987\n",
      "Epoch 70/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.1430e-04 - acc: 0.9999 - val_loss: 0.0039 - val_acc: 0.9987\n",
      "Epoch 71/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.7904e-04 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9987\n",
      "Epoch 72/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5159e-04 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9987\n",
      "Epoch 73/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.0882e-04 - acc: 0.9999 - val_loss: 0.0040 - val_acc: 0.9982\n",
      "Epoch 74/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.1041e-04 - acc: 0.9999 - val_loss: 0.0039 - val_acc: 0.9987\n",
      "Epoch 75/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.4999e-04 - acc: 0.9998 - val_loss: 0.0034 - val_acc: 0.9991\n",
      "Epoch 76/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.0826e-04 - acc: 0.9999 - val_loss: 0.0038 - val_acc: 0.9987\n",
      "Epoch 77/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.1937e-04 - acc: 0.9998 - val_loss: 0.0050 - val_acc: 0.9978\n",
      "Epoch 78/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0110 - acc: 0.9961 - val_loss: 0.0159 - val_acc: 0.9960\n",
      "Epoch 79/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0042 - val_acc: 0.9991\n",
      "Epoch 80/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0025 - val_acc: 0.9991\n",
      "Epoch 81/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.3859e-04 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9991\n",
      "Epoch 82/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.1738e-04 - acc: 0.9999 - val_loss: 0.0015 - val_acc: 0.9991\n",
      "Epoch 83/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.2111e-04 - acc: 0.9999 - val_loss: 0.0023 - val_acc: 0.9991\n",
      "Epoch 84/20000\n",
      "135/135 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9993  - 0s 3ms/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0075 - val_acc: 0.9960\n",
      "Epoch 85/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0069 - val_acc: 0.9978\n",
      "Epoch 86/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0064 - val_acc: 0.9973\n",
      "Epoch 87/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9986 - val_loss: 0.0096 - val_acc: 0.9964\n",
      "Epoch 88/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.0090 - val_acc: 0.9978\n",
      "Epoch 89/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.2335e-04 - acc: 0.9999 - val_loss: 0.0108 - val_acc: 0.9978\n",
      "Epoch 90/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.9721e-04 - acc: 0.9998 - val_loss: 0.0065 - val_acc: 0.9969\n",
      "Epoch 91/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0125 - val_acc: 0.9969\n",
      "Epoch 92/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.9793e-04 - acc: 0.9998 - val_loss: 0.0092 - val_acc: 0.9982\n",
      "Epoch 93/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.2711e-04 - acc: 0.9999 - val_loss: 0.0096 - val_acc: 0.9978\n",
      "Epoch 94/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.0087e-04 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 0.9982\n",
      "Epoch 95/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 3.9936e-04 - acc: 0.9999 - val_loss: 0.0095 - val_acc: 0.9978\n",
      "Epoch 96/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 2.2643e-04 - acc: 1.0000 - val_loss: 0.0099 - val_acc: 0.9978\n",
      "Epoch 97/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.5504e-04 - acc: 0.9999 - val_loss: 0.0134 - val_acc: 0.9978\n",
      "Epoch 98/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0108 - acc: 0.9969 - val_loss: 0.0114 - val_acc: 0.9955\n",
      "Epoch 99/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.0168 - val_acc: 0.9947\n",
      "Epoch 100/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0051 - acc: 0.9991 - val_loss: 0.0123 - val_acc: 0.9973\n",
      "Epoch 101/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0130 - val_acc: 0.9955\n",
      "Epoch 102/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.9335e-04 - acc: 1.0000 - val_loss: 0.0127 - val_acc: 0.9973\n",
      "Epoch 103/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.3959e-04 - acc: 0.9999 - val_loss: 0.0111 - val_acc: 0.9982\n",
      "Epoch 104/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5700e-04 - acc: 1.0000 - val_loss: 0.0114 - val_acc: 0.9982\n",
      "Epoch 105/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.4054e-04 - acc: 1.0000 - val_loss: 0.0108 - val_acc: 0.9982\n",
      "Epoch 106/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3700e-04 - acc: 1.0000 - val_loss: 0.0110 - val_acc: 0.9982\n",
      "Epoch 107/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3066e-04 - acc: 0.9999 - val_loss: 0.0110 - val_acc: 0.9982\n",
      "Epoch 108/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2264e-04 - acc: 0.9999 - val_loss: 0.0098 - val_acc: 0.9982\n",
      "Epoch 109/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8342e-04 - acc: 1.0000 - val_loss: 0.0094 - val_acc: 0.9978\n",
      "Epoch 110/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9696e-04 - acc: 1.0000 - val_loss: 0.0105 - val_acc: 0.9978\n",
      "Epoch 111/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.6849e-04 - acc: 1.0000 - val_loss: 0.0116 - val_acc: 0.9982\n",
      "Epoch 112/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5994e-04 - acc: 0.9999 - val_loss: 0.0096 - val_acc: 0.9982\n",
      "Epoch 113/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.5894e-04 - acc: 0.9999 - val_loss: 0.0125 - val_acc: 0.9978\n",
      "Epoch 114/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.1093 - val_acc: 0.9813\n",
      "Epoch 115/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0121 - acc: 0.9964 - val_loss: 0.0241 - val_acc: 0.9964\n",
      "Epoch 116/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.0118 - val_acc: 0.9973\n",
      "Epoch 117/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0072 - val_acc: 0.9978\n",
      "Epoch 118/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.0772e-04 - acc: 0.9999 - val_loss: 0.0065 - val_acc: 0.9978\n",
      "Epoch 119/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.3403e-04 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 0.9978\n",
      "Epoch 120/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.5886e-04 - acc: 0.9999 - val_loss: 0.0055 - val_acc: 0.9978\n",
      "Epoch 121/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.2711e-04 - acc: 0.9999 - val_loss: 0.0060 - val_acc: 0.9978\n",
      "Epoch 122/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.4737e-04 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9978\n",
      "Epoch 123/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2070e-04 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 0.9978\n",
      "Epoch 1/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0019 - val_acc: 0.9991\n",
      "Epoch 2/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0081 - acc: 0.9971 - val_loss: 0.0045 - val_acc: 0.9991\n",
      "Epoch 3/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0117 - acc: 0.9961 - val_loss: 0.0077 - val_acc: 0.9982\n",
      "Epoch 4/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0029 - val_acc: 0.9987\n",
      "Epoch 5/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9997 - val_loss: 4.7256e-04 - val_acc: 1.0000\n",
      "Epoch 6/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.6966e-04 - acc: 1.0000 - val_loss: 6.0984e-04 - val_acc: 1.0000\n",
      "Epoch 7/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.4395e-04 - acc: 1.0000 - val_loss: 4.3164e-04 - val_acc: 1.0000\n",
      "Epoch 8/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.7851e-04 - acc: 1.0000 - val_loss: 5.5367e-04 - val_acc: 1.0000\n",
      "Epoch 9/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.4604e-04 - acc: 0.9999 - val_loss: 4.1393e-04 - val_acc: 1.0000\n",
      "Epoch 10/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 1.9329e-04 - acc: 1.0000 - val_loss: 3.4848e-04 - val_acc: 1.0000\n",
      "Epoch 11/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 1.9096e-04 - acc: 1.0000 - val_loss: 9.0388e-04 - val_acc: 0.9996\n",
      "Epoch 12/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 2.3859e-04 - acc: 1.0000 - val_loss: 3.0370e-04 - val_acc: 1.0000\n",
      "Epoch 13/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.6228e-04 - acc: 0.9999 - val_loss: 5.7760e-04 - val_acc: 0.9996\n",
      "Epoch 14/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0042 - acc: 0.9985 - val_loss: 0.0692 - val_acc: 0.9889\n",
      "Epoch 15/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0169 - val_acc: 0.9964\n",
      "Epoch 16/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0141 - val_acc: 0.9960\n",
      "Epoch 17/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 7.8741e-04 - val_acc: 1.0000\n",
      "Epoch 18/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.2395e-04 - acc: 0.9999 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 19/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.4786e-04 - acc: 0.9999 - val_loss: 6.6809e-04 - val_acc: 1.0000\n",
      "Epoch 20/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5296e-04 - acc: 1.0000 - val_loss: 6.6617e-04 - val_acc: 1.0000\n",
      "Epoch 21/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.8804e-04 - acc: 0.9999 - val_loss: 8.8462e-04 - val_acc: 0.9996\n",
      "Epoch 22/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.9405e-04 - acc: 0.9999 - val_loss: 8.5967e-04 - val_acc: 1.0000\n",
      "Epoch 23/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.1544e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Epoch 24/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0133 - acc: 0.9970 - val_loss: 0.0036 - val_acc: 0.9991\n",
      "Epoch 25/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0027 - val_acc: 0.9987\n",
      "Epoch 26/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0058 - val_acc: 0.9978\n",
      "Epoch 27/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0020 - acc: 0.9991 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Epoch 28/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.2096e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Epoch 29/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5873e-04 - acc: 1.0000 - val_loss: 7.4553e-04 - val_acc: 1.0000\n",
      "Epoch 30/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1189e-04 - acc: 1.0000 - val_loss: 6.7691e-04 - val_acc: 1.0000\n",
      "Epoch 31/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9279e-04 - acc: 1.0000 - val_loss: 7.1598e-04 - val_acc: 1.0000\n",
      "Epoch 32/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 2.3325e-04 - acc: 0.9999 - val_loss: 6.4666e-04 - val_acc: 1.0000\n",
      "Epoch 33/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8062e-04 - acc: 1.0000 - val_loss: 6.3255e-04 - val_acc: 1.0000\n",
      "Epoch 34/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8301e-04 - acc: 1.0000 - val_loss: 4.5427e-04 - val_acc: 1.0000\n",
      "Epoch 35/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3234e-04 - acc: 1.0000 - val_loss: 9.2121e-04 - val_acc: 1.0000\n",
      "Epoch 36/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.3067e-04 - acc: 0.9998 - val_loss: 0.0042 - val_acc: 0.9978\n",
      "Epoch 37/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0121 - val_acc: 0.9947\n",
      "Epoch 38/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0302 - val_acc: 0.9933\n",
      "Epoch 39/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0028 - val_acc: 0.9987\n",
      "Epoch 40/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0041 - val_acc: 0.9991\n",
      "Epoch 41/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.3058e-04 - acc: 0.9999 - val_loss: 0.0025 - val_acc: 0.9991\n",
      "Epoch 42/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.9641e-04 - acc: 0.9999 - val_loss: 0.0016 - val_acc: 0.9991\n",
      "Epoch 43/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.6200e-04 - acc: 0.9999 - val_loss: 0.0030 - val_acc: 0.9996\n",
      "Epoch 44/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1962e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Epoch 45/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1075e-04 - acc: 0.9999 - val_loss: 9.5498e-04 - val_acc: 1.0000\n",
      "Epoch 46/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8675e-04 - acc: 1.0000 - val_loss: 9.2840e-04 - val_acc: 1.0000\n",
      "Epoch 47/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0456e-04 - acc: 1.0000 - val_loss: 8.1830e-04 - val_acc: 1.0000\n",
      "Epoch 48/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5691e-04 - acc: 0.9999 - val_loss: 8.6377e-04 - val_acc: 1.0000\n",
      "Epoch 49/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7316e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9996\n",
      "Epoch 50/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8780e-04 - acc: 1.0000 - val_loss: 8.3164e-04 - val_acc: 1.0000\n",
      "Epoch 51/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.9829e-04 - acc: 0.9999 - val_loss: 6.5605e-04 - val_acc: 1.0000\n",
      "Epoch 52/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0434e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9996\n",
      "Epoch 53/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0062 - acc: 0.9980 - val_loss: 0.0300 - val_acc: 0.9929\n",
      "Epoch 54/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0120 - acc: 0.9960 - val_loss: 0.0165 - val_acc: 0.9960\n",
      "Epoch 55/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0029 - val_acc: 0.9987\n",
      "Epoch 56/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.1464e-04 - acc: 0.9998 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Epoch 57/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.4548e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Epoch 58/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1952e-04 - acc: 0.9999 - val_loss: 0.0012 - val_acc: 0.9996\n",
      "Epoch 59/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1087e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9991\n",
      "Epoch 60/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3732e-04 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9987\n",
      "Epoch 61/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.6301e-04 - acc: 1.0000 - val_loss: 6.6666e-04 - val_acc: 1.0000\n",
      "Epoch 62/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.6547e-04 - acc: 0.9999 - val_loss: 5.9508e-04 - val_acc: 1.0000\n",
      "Epoch 63/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.4260e-04 - acc: 0.9999 - val_loss: 7.3112e-04 - val_acc: 1.0000\n",
      "Epoch 64/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0989e-04 - acc: 0.9999 - val_loss: 7.0400e-04 - val_acc: 1.0000\n",
      "Epoch 65/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.2695e-04 - acc: 0.9999 - val_loss: 0.0023 - val_acc: 0.9987\n",
      "Epoch 66/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 2.6773e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Epoch 67/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0070 - val_acc: 0.9964\n",
      "Epoch 68/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0092 - acc: 0.9971 - val_loss: 0.0214 - val_acc: 0.9933\n",
      "Epoch 69/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0026 - val_acc: 0.9991\n",
      "Epoch 70/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.0459e-04 - acc: 0.9998 - val_loss: 0.0012 - val_acc: 0.9996\n",
      "Epoch 71/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.5046e-04 - acc: 0.9999 - val_loss: 7.5079e-04 - val_acc: 1.0000\n",
      "Epoch 72/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.0794e-04 - acc: 0.9998 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Epoch 73/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.8648e-04 - acc: 0.9999 - val_loss: 8.3194e-04 - val_acc: 0.9996\n",
      "Epoch 74/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2028e-04 - acc: 0.9999 - val_loss: 6.7534e-04 - val_acc: 1.0000\n",
      "Epoch 75/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3928e-04 - acc: 0.9999 - val_loss: 6.5617e-04 - val_acc: 1.0000\n",
      "Epoch 76/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3863e-04 - acc: 0.9999 - val_loss: 0.0012 - val_acc: 0.9996\n",
      "Epoch 77/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0096 - val_acc: 0.9960\n",
      "Epoch 78/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9982\n",
      "Epoch 79/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.2379e-04 - acc: 0.9996 - val_loss: 0.0079 - val_acc: 0.9960\n",
      "Epoch 80/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0078 - acc: 0.9975 - val_loss: 0.0119 - val_acc: 0.9955\n",
      "Epoch 81/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0042 - val_acc: 0.9973\n",
      "Epoch 82/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.1243e-04 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9982\n",
      "Epoch 83/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.4285e-04 - acc: 0.9998 - val_loss: 0.0020 - val_acc: 0.9982\n",
      "Epoch 84/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2587e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 85/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.5188e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 86/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7399e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9987\n",
      "Epoch 87/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7310e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9991\n",
      "Epoch 88/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0590e-04 - acc: 0.9999 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Epoch 89/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3449e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Epoch 90/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.5149e-04 - acc: 0.9997 - val_loss: 0.0031 - val_acc: 0.9991\n",
      "Epoch 91/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0171 - acc: 0.9950 - val_loss: 0.0148 - val_acc: 0.9951\n",
      "Epoch 92/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0027 - val_acc: 0.9991\n",
      "Epoch 93/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.2490e-04 - acc: 0.9996 - val_loss: 0.0033 - val_acc: 0.9991\n",
      "Epoch 94/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.8459e-04 - acc: 0.9997 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Epoch 95/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2431e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 96/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9272e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Epoch 97/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7785e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Epoch 98/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7554e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9996\n",
      "Epoch 99/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8139e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 100/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8358e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9991\n",
      "Epoch 101/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1297e-04 - acc: 0.9999 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 102/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3761e-04 - acc: 0.9999 - val_loss: 0.0065 - val_acc: 0.9969\n",
      "Epoch 103/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 9.7934e-04 - acc: 0.9996 - val_loss: 0.0045 - val_acc: 0.9978\n",
      "Epoch 104/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0236 - val_acc: 0.9955\n",
      "Epoch 105/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0099 - acc: 0.9973 - val_loss: 0.0107 - val_acc: 0.9969\n",
      "Epoch 106/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0038 - val_acc: 0.9987\n",
      "Epoch 107/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.9913e-04 - acc: 0.9999 - val_loss: 0.0025 - val_acc: 0.9987\n",
      "Epoch 108/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2017e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9991\n",
      "Epoch 109/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9788e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 110/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0215e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9987\n",
      "Epoch 111/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7172e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9991\n",
      "Epoch 112/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.4480e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9987\n",
      "Epoch 1/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Epoch 2/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0026 - val_acc: 0.9996\n",
      "Epoch 3/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0021 - val_acc: 0.9991\n",
      "Epoch 4/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0097 - acc: 0.9971 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Epoch 5/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.0020 - val_acc: 0.9987\n",
      "Epoch 6/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0011 - val_acc: 0.9996\n",
      "Epoch 7/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.3831e-04 - acc: 0.9999 - val_loss: 6.2926e-04 - val_acc: 0.9996\n",
      "Epoch 8/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.9222e-04 - acc: 1.0000 - val_loss: 2.1041e-04 - val_acc: 1.0000\n",
      "Epoch 9/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 0.0016 - val_acc: 0.9991\n",
      "Epoch 10/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.1543e-04 - acc: 0.9999 - val_loss: 9.1246e-04 - val_acc: 0.9996\n",
      "Epoch 11/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5376e-04 - acc: 1.0000 - val_loss: 2.6218e-04 - val_acc: 1.0000\n",
      "Epoch 12/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0304e-04 - acc: 1.0000 - val_loss: 1.3051e-04 - val_acc: 1.0000\n",
      "Epoch 13/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 2.2081e-04 - acc: 0.9999 - val_loss: 1.0958e-04 - val_acc: 1.0000\n",
      "Epoch 14/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 1.5653e-04 - acc: 1.0000 - val_loss: 1.1693e-04 - val_acc: 1.0000\n",
      "Epoch 15/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7125e-04 - acc: 0.9999 - val_loss: 1.0068e-04 - val_acc: 1.0000\n",
      "Epoch 16/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8114e-04 - acc: 1.0000 - val_loss: 9.7874e-05 - val_acc: 1.0000\n",
      "Epoch 17/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.2450e-04 - acc: 0.9999 - val_loss: 8.6838e-05 - val_acc: 1.0000\n",
      "Epoch 18/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9911e-04 - acc: 0.9999 - val_loss: 9.4923e-05 - val_acc: 1.0000\n",
      "Epoch 19/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.6368e-04 - acc: 1.0000 - val_loss: 1.0985e-04 - val_acc: 1.0000\n",
      "Epoch 20/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9088e-04 - acc: 1.0000 - val_loss: 1.4195e-04 - val_acc: 1.0000\n",
      "Epoch 21/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9206e-04 - acc: 0.9999 - val_loss: 2.0521e-04 - val_acc: 1.0000\n",
      "Epoch 22/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1514e-04 - acc: 0.9999 - val_loss: 0.0022 - val_acc: 0.9987\n",
      "Epoch 23/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.0204 - acc: 0.9945 - val_loss: 0.0043 - val_acc: 0.9978\n",
      "Epoch 24/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0032 - val_acc: 0.9987\n",
      "Epoch 25/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.0019 - val_acc: 0.9991\n",
      "Epoch 26/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.1392e-04 - acc: 0.9998 - val_loss: 3.9444e-04 - val_acc: 1.0000\n",
      "Epoch 27/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.1735e-04 - acc: 0.9999 - val_loss: 1.8293e-04 - val_acc: 1.0000\n",
      "Epoch 28/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 2.1742e-04 - acc: 1.0000 - val_loss: 2.9065e-04 - val_acc: 1.0000\n",
      "Epoch 29/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 2.0746e-04 - acc: 1.0000 - val_loss: 1.4523e-04 - val_acc: 1.0000\n",
      "Epoch 30/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 1.6297e-04 - acc: 1.0000 - val_loss: 1.2953e-04 - val_acc: 1.0000\n",
      "Epoch 31/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0981e-04 - acc: 0.9999 - val_loss: 1.7297e-04 - val_acc: 1.0000\n",
      "Epoch 32/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1269e-04 - acc: 1.0000 - val_loss: 1.2801e-04 - val_acc: 1.0000\n",
      "Epoch 33/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8094e-04 - acc: 1.0000 - val_loss: 1.9468e-04 - val_acc: 1.0000\n",
      "Epoch 34/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3967e-04 - acc: 1.0000 - val_loss: 1.1802e-04 - val_acc: 1.0000\n",
      "Epoch 35/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2991e-04 - acc: 0.9999 - val_loss: 1.3586e-04 - val_acc: 1.0000\n",
      "Epoch 36/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0732e-04 - acc: 1.0000 - val_loss: 1.1894e-04 - val_acc: 1.0000\n",
      "Epoch 37/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9915e-04 - acc: 1.0000 - val_loss: 3.3839e-04 - val_acc: 1.0000\n",
      "Epoch 38/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 2.1256e-04 - acc: 1.0000 - val_loss: 9.9508e-05 - val_acc: 1.0000\n",
      "Epoch 39/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 1.8473e-04 - acc: 1.0000 - val_loss: 1.0435e-04 - val_acc: 1.0000\n",
      "Epoch 40/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0033 - val_acc: 0.9982\n",
      "Epoch 41/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0185 - acc: 0.9949 - val_loss: 0.0083 - val_acc: 0.9973\n",
      "Epoch 42/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0039 - acc: 0.9984 - val_loss: 0.0024 - val_acc: 0.9996\n",
      "Epoch 43/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.2040e-04 - acc: 0.9999 - val_loss: 0.0021 - val_acc: 0.9991\n",
      "Epoch 44/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.8661e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9991\n",
      "Epoch 45/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3075e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9991\n",
      "Epoch 46/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2852e-04 - acc: 1.0000 - val_loss: 9.7846e-04 - val_acc: 0.9996\n",
      "Epoch 47/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8028e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 0.9996\n",
      "Epoch 48/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7145e-04 - acc: 1.0000 - val_loss: 9.6328e-04 - val_acc: 0.9996\n",
      "Epoch 49/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.5197e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 0.9996\n",
      "Epoch 50/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0284e-04 - acc: 1.0000 - val_loss: 8.1174e-04 - val_acc: 0.9996\n",
      "Epoch 51/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0639e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9996\n",
      "Epoch 52/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.5412e-04 - acc: 0.9999 - val_loss: 8.2370e-04 - val_acc: 0.9996\n",
      "Epoch 53/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0779e-04 - acc: 0.9999 - val_loss: 8.9592e-04 - val_acc: 0.9996\n",
      "Epoch 54/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5896e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9991\n",
      "Epoch 55/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5919e-04 - acc: 0.9999 - val_loss: 0.0010 - val_acc: 0.9991\n",
      "Epoch 56/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5446e-04 - acc: 0.9999 - val_loss: 0.0011 - val_acc: 0.9996\n",
      "Epoch 57/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5474e-04 - acc: 0.9999 - val_loss: 0.0014 - val_acc: 0.9987\n",
      "Epoch 58/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3377e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9996\n",
      "Epoch 59/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0278 - val_acc: 0.9915\n",
      "Epoch 60/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0083 - acc: 0.9970 - val_loss: 0.0089 - val_acc: 0.9978\n",
      "Epoch 61/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0054 - val_acc: 0.9987\n",
      "Epoch 62/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0029 - val_acc: 0.9991\n",
      "Epoch 63/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.2156e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9996\n",
      "Epoch 64/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.2205e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9996\n",
      "Epoch 65/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2755e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9996\n",
      "Epoch 66/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9972e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Epoch 67/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2015e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9996\n",
      "Epoch 68/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7547e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9996\n",
      "Epoch 69/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7091e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9996\n",
      "Epoch 70/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1586e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9996\n",
      "Epoch 71/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8094e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Epoch 72/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.4663e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Epoch 73/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.6509e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9996\n",
      "Epoch 74/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3806e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9996\n",
      "Epoch 75/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.6086e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Epoch 76/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0090 - acc: 0.9974 - val_loss: 0.0171 - val_acc: 0.9942\n",
      "Epoch 77/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0068 - acc: 0.9978 - val_loss: 0.0040 - val_acc: 0.9978\n",
      "Epoch 78/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 79/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.3439e-04 - acc: 0.9999 - val_loss: 0.0021 - val_acc: 0.9991\n",
      "Epoch 80/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.1115e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9996\n",
      "Epoch 81/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.2578e-04 - acc: 0.9999 - val_loss: 0.0056 - val_acc: 0.9982\n",
      "Epoch 82/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.1364e-04 - acc: 0.9999 - val_loss: 0.0041 - val_acc: 0.9991\n",
      "Epoch 83/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1404e-04 - acc: 0.9999 - val_loss: 0.0042 - val_acc: 0.9991\n",
      "Epoch 84/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7239e-04 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9991\n",
      "Epoch 85/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9473e-04 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9991\n",
      "Epoch 86/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2489e-04 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 0.9991\n",
      "Epoch 87/20000\n",
      "135/135 [==============================] - ETA: 0s - loss: 3.3875e-04 - acc: 0.999 - 0s 3ms/step - loss: 3.2311e-04 - acc: 0.9999 - val_loss: 0.0035 - val_acc: 0.9991\n",
      "Epoch 88/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0401e-04 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9991\n",
      "Epoch 89/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0106 - acc: 0.9972 - val_loss: 0.0259 - val_acc: 0.9924\n",
      "Epoch 90/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.0071 - val_acc: 0.9982\n",
      "Epoch 91/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.6813e-04 - acc: 0.9997 - val_loss: 0.0079 - val_acc: 0.9973\n",
      "Epoch 92/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.8552e-04 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 0.9982\n",
      "Epoch 93/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2216e-04 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9982\n",
      "Epoch 94/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.1540e-04 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 0.9987\n",
      "Epoch 95/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0851e-04 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 0.9991\n",
      "Epoch 96/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.4014e-04 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 0.9991\n",
      "Epoch 97/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.4919e-04 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 0.9987\n",
      "Epoch 98/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3088e-04 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9982\n",
      "Epoch 99/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 6.0606e-04 - acc: 0.9997 - val_loss: 0.0060 - val_acc: 0.9987\n",
      "Epoch 100/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0588e-04 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 0.9991\n",
      "Epoch 101/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 2.0582e-04 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 0.9991\n",
      "Epoch 102/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8498e-04 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 0.9991\n",
      "Epoch 103/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0752e-04 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 0.9991\n",
      "Epoch 104/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.3302e-04 - acc: 0.9999 - val_loss: 0.0068 - val_acc: 0.9982\n",
      "Epoch 105/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.1058e-04 - acc: 0.9999 - val_loss: 0.0055 - val_acc: 0.9982\n",
      "Epoch 106/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0243 - val_acc: 0.9938\n",
      "Epoch 107/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0107 - acc: 0.9968 - val_loss: 0.0116 - val_acc: 0.9960\n",
      "Epoch 108/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9987 - val_loss: 0.0050 - val_acc: 0.9991\n",
      "Epoch 109/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.3133e-04 - acc: 0.9999 - val_loss: 0.0041 - val_acc: 0.9987\n",
      "Epoch 110/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.5054e-04 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 0.9996\n",
      "Epoch 111/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0691e-04 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9991\n",
      "Epoch 112/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2017e-04 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9991\n",
      "Epoch 113/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.6513e-04 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9991\n",
      "Epoch 114/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.5136e-04 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9991\n",
      "Epoch 115/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7773e-04 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9991\n",
      "Epoch 116/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3428e-04 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9991\n",
      "Epoch 117/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0896e-04 - acc: 0.9999 - val_loss: 0.0043 - val_acc: 0.9991\n",
      "Epoch 1/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 7.5779e-04 - acc: 0.9998 - val_loss: 5.4045e-04 - val_acc: 0.9996\n",
      "Epoch 2/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0092 - val_acc: 0.9987\n",
      "Epoch 3/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0030 - val_acc: 0.9987\n",
      "Epoch 4/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0043 - acc: 0.9982 - val_loss: 0.0018 - val_acc: 0.9991\n",
      "Epoch 5/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 5.1138e-04 - val_acc: 1.0000\n",
      "Epoch 6/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2303e-04 - acc: 1.0000 - val_loss: 4.1317e-04 - val_acc: 1.0000\n",
      "Epoch 7/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.8603e-04 - acc: 1.0000 - val_loss: 5.8624e-04 - val_acc: 0.9996\n",
      "Epoch 8/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.6077e-04 - acc: 1.0000 - val_loss: 3.4880e-04 - val_acc: 1.0000\n",
      "Epoch 9/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.5429e-04 - acc: 1.0000 - val_loss: 3.3300e-04 - val_acc: 1.0000\n",
      "Epoch 10/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 1.4943e-04 - acc: 1.0000 - val_loss: 3.5123e-04 - val_acc: 1.0000\n",
      "Epoch 11/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.4093e-04 - acc: 1.0000 - val_loss: 8.3893e-04 - val_acc: 0.9996\n",
      "Epoch 12/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 1.3457e-04 - acc: 1.0000 - val_loss: 4.6725e-04 - val_acc: 0.9996\n",
      "Epoch 13/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3660e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9996\n",
      "Epoch 14/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9891e-04 - acc: 1.0000 - val_loss: 6.7798e-04 - val_acc: 0.9996\n",
      "Epoch 15/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3084e-04 - acc: 1.0000 - val_loss: 3.6645e-04 - val_acc: 1.0000\n",
      "Epoch 16/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0070 - val_acc: 0.9969\n",
      "Epoch 17/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.0060 - acc: 0.9979 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Epoch 18/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 9.1281e-04 - val_acc: 0.9996\n",
      "Epoch 19/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.0039e-04 - acc: 0.9999 - val_loss: 0.0022 - val_acc: 0.9991\n",
      "Epoch 20/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 9.2166e-04 - val_acc: 0.9996\n",
      "Epoch 21/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.8264e-04 - acc: 1.0000 - val_loss: 4.7959e-04 - val_acc: 1.0000\n",
      "Epoch 22/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 2.6419e-04 - acc: 0.9999 - val_loss: 8.2742e-04 - val_acc: 0.9996\n",
      "Epoch 23/20000\n",
      "135/135 [==============================] - 0s 2ms/step - loss: 2.6572e-04 - acc: 0.9999 - val_loss: 3.9741e-04 - val_acc: 1.0000\n",
      "Epoch 24/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.2330e-04 - acc: 1.0000 - val_loss: 2.2301e-04 - val_acc: 1.0000\n",
      "Epoch 25/20000\n",
      "135/135 [==============================] - 1s 8ms/step - loss: 1.1436e-04 - acc: 1.0000 - val_loss: 5.5061e-04 - val_acc: 0.9996\n",
      "Epoch 26/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 1.0711e-04 - acc: 1.0000 - val_loss: 3.7616e-04 - val_acc: 1.0000\n",
      "Epoch 27/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 1.3342e-04 - acc: 1.0000 - val_loss: 2.6355e-04 - val_acc: 1.0000\n",
      "Epoch 28/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 1.6514e-04 - acc: 1.0000 - val_loss: 5.0845e-04 - val_acc: 0.9996\n",
      "Epoch 29/20000\n",
      "135/135 [==============================] - 1s 7ms/step - loss: 1.3187e-04 - acc: 1.0000 - val_loss: 2.4419e-04 - val_acc: 1.0000\n",
      "Epoch 30/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 1.7473e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9991\n",
      "Epoch 31/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0319 - val_acc: 0.9933\n",
      "Epoch 32/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0131 - acc: 0.9964 - val_loss: 0.0142 - val_acc: 0.9938\n",
      "Epoch 33/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0027 - val_acc: 0.9991\n",
      "Epoch 34/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.9727e-04 - acc: 0.9998 - val_loss: 0.0012 - val_acc: 0.9996\n",
      "Epoch 35/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 4.8972e-04 - acc: 0.9999 - val_loss: 9.4164e-04 - val_acc: 0.9996\n",
      "Epoch 36/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.9112e-04 - acc: 0.9999 - val_loss: 8.6603e-04 - val_acc: 0.9996\n",
      "Epoch 37/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.6030e-04 - acc: 1.0000 - val_loss: 7.3523e-04 - val_acc: 1.0000\n",
      "Epoch 38/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.1680e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9996\n",
      "Epoch 39/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3256e-04 - acc: 1.0000 - val_loss: 6.0880e-04 - val_acc: 0.9996\n",
      "Epoch 40/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.5173e-04 - acc: 1.0000 - val_loss: 6.6919e-04 - val_acc: 0.9996\n",
      "Epoch 41/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.4607e-04 - acc: 1.0000 - val_loss: 5.6777e-04 - val_acc: 0.9996\n",
      "Epoch 42/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.6913e-05 - acc: 1.0000 - val_loss: 5.9512e-04 - val_acc: 0.9996\n",
      "Epoch 43/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9423e-04 - acc: 1.0000 - val_loss: 6.3743e-04 - val_acc: 1.0000\n",
      "Epoch 44/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0054 - val_acc: 0.9978\n",
      "Epoch 45/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0071 - acc: 0.9982 - val_loss: 0.0138 - val_acc: 0.9955\n",
      "Epoch 46/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9990 - val_loss: 0.0024 - val_acc: 0.9991\n",
      "Epoch 47/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 7.4854e-04 - acc: 0.9998 - val_loss: 0.0022 - val_acc: 0.9987\n",
      "Epoch 48/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 2.0339e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9991\n",
      "Epoch 49/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.2762e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9991\n",
      "Epoch 50/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.1091e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 51/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3977e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 52/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 2.3285e-04 - acc: 0.9999 - val_loss: 0.0022 - val_acc: 0.9991\n",
      "Epoch 53/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0395e-04 - acc: 1.0000 - val_loss: 8.3366e-04 - val_acc: 1.0000\n",
      "Epoch 54/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7157e-04 - acc: 0.9999 - val_loss: 0.0011 - val_acc: 0.9991\n",
      "Epoch 55/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3176e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9991\n",
      "Epoch 56/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7026e-04 - acc: 1.0000 - val_loss: 9.8968e-04 - val_acc: 0.9991\n",
      "Epoch 57/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.2078e-05 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9991\n",
      "Epoch 58/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.6549e-04 - acc: 0.9999 - val_loss: 0.0020 - val_acc: 0.9991\n",
      "Epoch 59/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0059 - acc: 0.9984 - val_loss: 0.0152 - val_acc: 0.9964\n",
      "Epoch 60/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0192 - acc: 0.9956 - val_loss: 0.0135 - val_acc: 0.9947\n",
      "Epoch 61/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Epoch 62/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.2934e-04 - acc: 0.9998 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 63/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.8525e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9996\n",
      "Epoch 64/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.5834e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9996\n",
      "Epoch 65/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.4004e-04 - acc: 1.0000 - val_loss: 9.6478e-04 - val_acc: 0.9996\n",
      "Epoch 66/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3049e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9996\n",
      "Epoch 67/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.4839e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9996\n",
      "Epoch 68/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 1.4221e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9991\n",
      "Epoch 69/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3281e-04 - acc: 1.0000 - val_loss: 9.5829e-04 - val_acc: 0.9996\n",
      "Epoch 70/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.2549e-04 - acc: 1.0000 - val_loss: 7.4914e-04 - val_acc: 1.0000\n",
      "Epoch 71/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7640e-04 - acc: 0.9999 - val_loss: 8.7588e-04 - val_acc: 0.9996\n",
      "Epoch 72/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.5309e-05 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Epoch 73/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3415e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9996\n",
      "Epoch 74/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 9.9122e-05 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9991\n",
      "Epoch 75/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.0178e-04 - acc: 1.0000 - val_loss: 8.7514e-04 - val_acc: 0.9996\n",
      "Epoch 76/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2842e-04 - acc: 0.9999 - val_loss: 0.0011 - val_acc: 0.9996\n",
      "Epoch 77/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3646e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9991\n",
      "Epoch 78/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3479e-04 - acc: 1.0000 - val_loss: 7.4972e-04 - val_acc: 1.0000\n",
      "Epoch 79/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.2198e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9996\n",
      "Epoch 80/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 6.9998e-04 - acc: 0.9997 - val_loss: 0.0130 - val_acc: 0.9973\n",
      "Epoch 81/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0137 - acc: 0.9962 - val_loss: 0.0136 - val_acc: 0.9964\n",
      "Epoch 82/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0043 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 0.9987\n",
      "Epoch 83/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 6.6844e-04 - acc: 0.9998 - val_loss: 0.0023 - val_acc: 0.9991\n",
      "Epoch 84/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2714e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9991\n",
      "Epoch 85/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.9000e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 86/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.2819e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9991\n",
      "Epoch 87/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.1622e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Epoch 88/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.1416e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9996\n",
      "Epoch 89/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 1.6630e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9996\n",
      "Epoch 90/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.0707e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Epoch 91/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.1187e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Epoch 92/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.3718e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Epoch 93/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.0833e-04 - acc: 0.9999 - val_loss: 0.0025 - val_acc: 0.9991\n",
      "Epoch 94/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0359 - val_acc: 0.9929\n",
      "Epoch 95/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0086 - acc: 0.9968 - val_loss: 0.0041 - val_acc: 0.9987\n",
      "Epoch 96/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0023 - val_acc: 0.9991\n",
      "Epoch 97/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 8.5480e-04 - acc: 0.9998 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Epoch 98/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.7062e-04 - acc: 0.9999 - val_loss: 0.0014 - val_acc: 0.9991\n",
      "Epoch 99/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8282e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9991\n",
      "Epoch 100/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 1.2065e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9991\n",
      "Epoch 101/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.0176e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9991\n",
      "Epoch 102/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.0564e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9987\n",
      "Epoch 103/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.8910e-04 - acc: 0.9999 - val_loss: 0.0019 - val_acc: 0.9991\n",
      "Epoch 104/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.1883e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9996\n",
      "Epoch 105/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.0386e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9987\n",
      "Epoch 106/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.1058e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9996\n",
      "Epoch 107/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 1.4758e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9991\n",
      "Epoch 108/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 7.5212e-05 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9991\n",
      "Epoch 109/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 7.3893e-05 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9991\n",
      "Epoch 110/20000\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 1.3157e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9987\n",
      "Epoch 111/20000\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 2.8163e-04 - acc: 0.9999 - val_loss: 0.0011 - val_acc: 0.9991\n",
      "Epoch 112/20000\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 2.8972e-04 - acc: 0.9999 - val_loss: 0.0015 - val_acc: 0.9991\n",
      "Epoch 113/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 0.0169 - acc: 0.9955 - val_loss: 0.0199 - val_acc: 0.9929\n",
      "Epoch 114/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0022 - val_acc: 0.9987\n",
      "Epoch 115/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 5.1237e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9996\n",
      "Epoch 116/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.6010e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9987\n",
      "Epoch 117/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.2577e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9987\n",
      "Epoch 118/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.1373e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9991\n",
      "Epoch 119/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.1449e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 120/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 1.4100e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 121/20000\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 1.3678e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9982\n",
      "Epoch 122/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0762e-04 - acc: 0.9999 - val_loss: 0.0021 - val_acc: 0.9987\n",
      "Epoch 123/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.1423e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9987\n",
      "Epoch 124/20000\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.4459e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9991\n",
      "\n",
      "Training time:\n",
      "575.3323016166687\n",
      "235/235 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.987051\n",
      "Precision: 0.977898\n",
      "Recall: 0.974547\n",
      "F1 score: 0.976220\n",
      "Loss: 0.207656\n",
      "Cohens kappa: 0.967323\n",
      "ROC AUC: 0.997093\n",
      "[[5403   45]\n",
      " [  52 1991]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.990     0.992     0.991      5448\n",
      "         1.0      0.978     0.975     0.976      2043\n",
      "\n",
      "    accuracy                          0.987      7491\n",
      "   macro avg      0.984     0.983     0.984      7491\n",
      "weighted avg      0.987     0.987     0.987      7491\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Training time:\n",
      "576.4410042762756\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_check = {\n",
    "\t\"base\": {\n",
    "\t\t\"features\"\t   : [4,5,6,7,8,9,10,11,12],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"base_robust\": {\n",
    "\t\t\"features\": [2,3,4,5,6,7,8,11,12,13,14],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"all\": {\n",
    "\t\t\"features\": [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"novel\": {\n",
    "\t\t\"features\": [13,14,15,16,17],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"base_robust\": {\n",
    "\t\t\"features\": [13,14,15,16,5,9,11,12],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"new_features\": {\n",
    "\t\t\"features\": [2,3],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"old\": {\n",
    "\t\t\"features\": [4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"new_robust\": {\n",
    "\t\t\"features\": [2,3,13,14,15,16,17],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"new_BR_robust\": {\n",
    "\t\t\"features\": [2,3,13,14,15,16,5,9,11,12,17],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"new_BR_robust_entropy\": {\n",
    "\t\t\"features\": [2,3,13,14,15,16,5,9,11,12,6,17],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"BR_robust_entropy\": {\n",
    "\t\t\"features\": [13,14,15,16,5,9,11,12,6,17],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"new_BR_robust_entropy_numip_distip_noccr\": {\n",
    "\t\t\"features\": [13,14,5,9,11,12,6,7,8,10,17],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t},\n",
    "\t\"BR_robust_entropy_numip_distip\": {\n",
    "\t\t\"features\": [13,14,15,16,5,9,11,12,6,7,8,10,17],\n",
    "\t\t\"y_column_idx\" : 1,\n",
    "\t\t\"feature_file\" : \"superFinalWithAds.csv\"\n",
    "\t}\n",
    "\n",
    "}\n",
    "#features_to_check = [\"base\",\"base_robust\",\"all\",\"novel\",\"hybrid_robust\",\"new_features\"]\n",
    "features_to_check = [\"new_BR_robust_entropy_numip_distip_noccr\"]\n",
    "\n",
    "threshold       = 0.5\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 20000\n",
    "degree          = 3\n",
    "n_splits\t\t= 10\n",
    "test_size\t\t= 0.25\n",
    "\n",
    "path               = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# features_file_name = \"../Datasets/features_extractions/median_9_2_(25-75)_vt_include.csv\"\n",
    "#for every variable in freatures_to_check (We have 5) we will check if we can find a malicious sign. if so we will append the url to df.\n",
    "for features_set in features_to_check:\n",
    "\tprint(\"\\n\\nChecking features - %s\" % (features_set))\n",
    "\tfeatures_file = features_check[features_set][\"feature_file\"]\n",
    "\ty_column_idx  = features_check[features_set][\"y_column_idx\"]\n",
    "\tstart         = time.time()\n",
    "\tdf            = pd.read_csv(features_file)\n",
    "\tdf.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "\t######## Append artificial data by number of consecutive characters feature ########\n",
    "\tif 5 in features_check[features_set][\"features\"]:\n",
    "\t\ttemp =  df[df[df.columns[y_column_idx]]==1]\n",
    "\t\t# try:\n",
    "\t\tmal         = temp.sample(500).copy()\n",
    "\t\tmal[\"5\"]    = mal[\"5\"].apply(lambda x:x*random.randint(3,9))\n",
    "\t\tdf = df.append(mal, ignore_index=True)\n",
    "\t\t# except Exception as e:\n",
    "\t\t# \tprint(e)\n",
    "\t######################################## END #######################################\n",
    "\tuse_columns   = features_check[features_set][\"features\"]\n",
    "\tuse_columns.append(y_column_idx)\n",
    "\n",
    "\tnew_df = df[df.columns[use_columns]]\n",
    "\tnew_df = np.array(new_df.values)\n",
    "\t#create new neural network\n",
    "\tnn     = NeuralNetwork(dataset=new_df, learning_rate=learning_rate, threshold=threshold, kfolds=n_splits, training_epochs=training_epochs, degree=degree)\n",
    "\t# build the nn, train it and try to predict\n",
    "\tnn.build()\n",
    "\tnn.train(verbose=1)\n",
    "\tscores = nn.predict()\n",
    "\tnn.save_model(\"ann_model_base_t\")\n",
    "\n",
    "\tend   = time.time()\n",
    "\tprint(\"\\nTraining time:\")\n",
    "\tprint(end - start)\n",
    "\tnn.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "ner2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "d07650068d61125eebdc0d5f99de5106bd98522aef3474d27d944b0e7503d5e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
